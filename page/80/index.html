<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2017/07/31/readingtipsonlinuxsystemarchitecture/>ReadingTipsOnLinuxSystemArchitecture</a></h1><span class=post-date>Jul 31, 2017<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=on-this-book>On This Book</h3><p>Borrowed from lab, written via a janpanese author.<br><img src=/images/2017_07_31_09_20_33_1054x739.jpg alt=/images/2017_07_31_09_20_33_1054x739.jpg>
This article will record the reading tips on Chapter 2(libvirtd related).</p><h3 id=network-configuration>Network Configuration</h3><p>Edit the netoworking definition xml:</p><pre><code>$ cat internal.xml
&lt;network&gt;
	&lt;name&gt;internal&lt;/name&gt;
	&lt;bridge name='virbr8'/&gt;
&lt;/network&gt;
$  cat external.xml
&lt;network&gt;
	&lt;name&gt;external&lt;/name&gt;
	&lt;bridge name='virbr9'/&gt;
&lt;/network&gt;
</code></pre><p>Define the networking via following commands:</p><pre><code>$ sudo virsh net-define external.xml
Network external defined from external.xml

$ sudo virsh net-autostart external
Network external marked as autostarted

$ sudo virsh net-start external
Network external started

$ libvirt sudo virsh net-list
 Name                 State      Autostart     Persistent
----------------------------------------------------------
 default              active     no            yes
 external             active     yes           yes
 internal             active     yes           yes
 kubernetes           active     yes           yes
</code></pre><p>View the configuration in virt-manager:</p><p><img src=/images/2017_07_31_09_34_07_495x298.jpg alt=/images/2017_07_31_09_34_07_495x298.jpg></p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2017/07/27/correcthugodate/>CorrectHugoDate</a></h1><span class=post-date>Jul 27, 2017<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=problem>Problem</h3><p><img src=/images/2017_07_27_16_08_05_1361x260.jpg alt=/images/2017_07_27_16_08_05_1361x260.jpg></p><h3 id=reason>Reason</h3><p>This is because hugo upgrade to a new version <code>0.25.1</code>, while this new version
won&rsquo;t give the default value of date in newly created markdown file.</p><h3 id=solution>Solution</h3><p>Edit the <code>themes/hyde-a/archetypes/default.md</code>, add following items:</p><pre><code>+++
title = &quot;&quot;
date = &quot;{{ .Date }}&quot;
description = &quot;&quot;
keywords = [&quot;Linux&quot;]
categories = [&quot;Technology&quot;]
+++
</code></pre><p>Now you could re-new your configuration, and then your blog will acts OK.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2017/07/27/createrhel6customizediso/>CreateRHEL6CustomizedISO</a></h1><span class=post-date>Jul 27, 2017<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=目的>目的</h3><p>根据用户自定义配置，自动从ISO安装出整个系统。</p><h3 id=准备材料>准备材料</h3><p>RHEL 6.6安装光盘, <code>x86_64</code>版本。
自定义kickstart文件，用于自定义分区/用户/密码/安装包等<br>红帽系列操作系统(用于制作光盘镜像，已验证Redhat7.3)</p><h3 id=步骤>步骤</h3><ol><li>创建目录用于挂载安装光盘和自定义光盘,
其中<code>/media/bootiso</code>用于挂载安装光盘，
<code>/media/bootisoks</code>用于存放自定义光盘内容:</li></ol><pre><code>$ mkdir -p /media/bootiso /media/bootisoks
</code></pre><ol start=2><li>拷贝安装内容到自定义光盘目录:</li></ol><pre><code>$ sudo mount -t iso9660 -o loop DVD.iso /media/bootiso
$ cp -r /media/bootiso/* /media/bootisoks/
$ chmdo -R u+w /media/bootisoks
$ cp /media/bootiso/.discinfo /media/bootisoks
$ cp /media/bootiso/.discinfo /media/bootisoks/isolinux
</code></pre><ol start=3><li>拷贝自定义的ks文件到isolinux目录下:</li></ol><pre><code>$ cp YourKickStartFile.ks /media/bootisoks/isolinux
</code></pre><ol start=4><li>配置引导选项:</li></ol><pre><code>$ vim /media/bootisoks/isolinux.cfg
initrd=initrd.img ks=cdrom:/isolinux/ks.cfg
</code></pre><ol start=5><li>创建ISO文件:</li></ol><pre><code># mkisofs -r -T -V &quot;MYISONAME&quot; -b isolinux/isolinux.bin -c isolinux/boot.cat
-no-emul-boot -boot-load-size 4 -boot-info-table -o ../boot.iso .
</code></pre><p>经历此五个步骤以后，即可得到我们定制好的ISO，用此ISO即可安装出我们自定义好的系统.</p><h3 id=kickstart示例文件>kickstart示例文件:</h3><p>安装了基本桌面、中文支持等。</p><pre><code>#platform=x86, AMD64, or Intel EM64T
#version=DEVEL
# Firewall configuration
firewall --disabled
# Install OS instead of upgrade
install
# Use network installation
#url --url=&quot;http://10.7.7.2/CentOS&quot;
cdrom
# Root password
rootpw --iscrypted xxxxxxxxxxxxxxxxxxxx
# System authorization information
auth  --useshadow  --passalgo=sha512
# Use graphical install
graphical
firstboot --disable
# System keyboard
keyboard us
# System language
lang en_US
# SELinux configuration
selinux --disabled
# Installation logging level
logging --level=info

# System timezone
timezone  Asia/Hong_Kong
# System bootloader configuration
bootloader --location=mbr
# Clear the Master Boot Record
zerombr
# Partition clearing information
clearpart --all  
# Disk partitioning information
part swap --fstype=&quot;swap&quot; --size=1024
part / --asprimary --fstype=&quot;ext4&quot; --grow --size=1

%packages
@basic-desktop
@chinese-support
@internet-browser
@x11
-ibus-table-cangjie
-ibus-table-erbi
-ibus-table-wubi

%end
</code></pre><p>其中<code>rootpw</code>以后的字段可以通过以下命令得到:</p><pre><code>$ openssl passwd -1 &quot;Your_Password_Here&quot;
</code></pre><h3 id=kscfg的另一种构建方法>ks.cfg的另一种构建方法</h3><p>在安装完的每一台机器上，都可以看到/root/ana&mldr;ks文件，编辑此文件即可得到我们定制化的kickstart配置。</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2017/07/21/workingtipsonoracledatabasedeployment/>WorkingTipsOnOracleDatabaseDeployment</a></h1><span class=post-date>Jul 21, 2017<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=items>Items</h3><p>Working items on one-click deployment of oracle database.</p><h3 id=ansible-playbooks>Ansible-Playbooks</h3><p>Based on:</p><p><a href=https://github.com/nkadbi/oracle-db-12c-vagrant-ansible>https://github.com/nkadbi/oracle-db-12c-vagrant-ansible</a></p><p>Refers to:</p><p><a href=https://blog.dbi-services.com/vagrant-up-get-your-oracle-infrastructure-up-and-running/>https://blog.dbi-services.com/vagrant-up-get-your-oracle-infrastructure-up-and-running/</a><br><a href=https://blog.dbi-services.com/part2-vagrant-up-get-your-oracle-infrastructure-up-an-running/>https://blog.dbi-services.com/part2-vagrant-up-get-your-oracle-infrastructure-up-an-running/</a></p><p>Username/Password:<br>System: oracle/welcome1<br>Database: sys/oracle</p><h3 id=linux-client>Linux Client</h3><p>Yaourt has the linux client for accessing oracle Db:</p><p><a href=https://aur.archlinux.org/packages/oracle-sqldeveloper/>https://aur.archlinux.org/packages/oracle-sqldeveloper/</a></p><p>Installing method:<br>Download the file from oracle.com</p><h3 id=create-database>Create Database</h3><p>Create database using following command:</p><pre><code>[vagrant@dbserver1 ~]$ su - oracle
Password: 
-bash-4.2$ sqlplus &quot;/as sysdba&quot;
</code></pre><p>Now you got the shell like <code>SQL></code>, you could input the sql in this shell:</p><pre><code>Run `1_create_user_and_tablespace_dash.sql`
</code></pre><h3 id=create-tablesmetadatas>Create tables/metadatas</h3><p>The first step will create the database user, then you could login into the
database using this user, using SQL Devloper for login and execute the
command:</p><p><img src=/images/2017_07_23_13_47_44_745x382.jpg alt=/images/2017_07_23_13_47_44_745x382.jpg></p><p>Execute the following script:</p><pre><code>msp_XXX.sql(Including 2 scripts)   
</code></pre><p><img src=/images/2017_07_23_13_50_23_506x466.jpg alt=/images/2017_07_23_13_50_23_506x466.jpg></p><p>Tips for getting the db config:</p><pre><code> SQL&gt; show parameter service_names;
.....
service_names			     string	 db1.private
</code></pre><p>Then your configuration should use the same <code>service_names</code> as described.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2017/07/13/dockernetworkperformancetest/>DockerNetworkPerformanceTest</a></h1><span class=post-date>Jul 13, 2017<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=测试环境>测试环境</h3><p>Docker常用的两种网络模式包括Bridge和Host模式，为测试这两种网络模式的性能，我们将创建以下的测试环境:</p><ul><li>192.192.192.89 - 运行Docker容器的服务器， CentOS 7.3.</li><li>192.192.192.88 - 运行客户端的服务器, CentOS 7.3.</li></ul><p>两台服务器之间的物理网络为万兆以太网络。</p><p>我们采用Iperf<a href=http://software.es.net/iperf/>http://software.es.net/iperf/</a>来测量网络带宽，iperf非常简单，也拥有足够多的特性用于测试基本的性能指标。
在服务器端，我们需要一个运行iperf3的Docker容器。 Docker的版本为17.05-ce.</p><p>测试将基于以下几个场景:</p><ul><li>原始网络吞吐量</li><li>跨主机物理机到Docker(host模式)</li><li>跨主机物理机到Docker(Bridge模式)</li><li>同主机物理机到Docker(Bridge模式)</li><li>同主机Docker到Docker(Bridge模式-external)</li><li>同主机Docker到Docker(Bridge模式-internal)</li></ul><h3 id=原始网络吞吐量>原始网络吞吐量</h3><p>首先，我们需要得到在没有任何Docker容器运行时的原始网络吞吐，在Server端运行:</p><pre><code>[root@192.192.192.89 ~]# iperf3 -s -p 5202
</code></pre><p>Client端运行:</p><pre><code>[root@192.192.192.88 ~]# iperf3 -c 192.192.192.89 -p 5202
</code></pre><p>运行测试后，服务器端和客户端都会返回诊断信息。我们暂时只关心其吞吐量:</p><pre><code>-----------------------------------------------------------
Server listening on 5202
-----------------------------------------------------------
Accepted connection from 192.192.192.88, port 39682
[  5] local 192.192.192.89 port 5202 connected to 192.192.192.88 port 39684
[ ID] Interval           Transfer     Bandwidth
[  5]   0.00-1.00   sec  1.05 GBytes  9.05 Gbits/sec                  
[  5]   1.00-2.00   sec  1.10 GBytes  9.41 Gbits/sec                  
[  5]   2.00-3.00   sec  1.10 GBytes  9.41 Gbits/sec                  
[  5]   3.00-4.00   sec  1.10 GBytes  9.41 Gbits/sec                  
[  5]   4.00-5.00   sec  1.10 GBytes  9.41 Gbits/sec                  
[  5]   5.00-6.00   sec  1.10 GBytes  9.41 Gbits/sec                  
[  5]   6.00-7.00   sec  1.10 GBytes  9.41 Gbits/sec                  
[  5]   7.00-8.00   sec  1.10 GBytes  9.41 Gbits/sec                  
[  5]   8.00-9.00   sec  1.10 GBytes  9.41 Gbits/sec                  
[  5]   9.00-10.00  sec  1.10 GBytes  9.41 Gbits/sec                  
[  5]  10.00-10.04  sec  42.0 MBytes  9.39 Gbits/sec                  
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bandwidth
[  5]   0.00-10.04  sec  0.00 Bytes  0.00 bits/sec                  sender
[  5]   0.00-10.04  sec  11.0 GBytes  9.38 Gbits/sec                  receiver
-----------------------------------------------------------
Server listening on 5202
-----------------------------------------------------------
</code></pre><p>可以看到，在万兆交换机的网络场景下，物理机到物理机之间的网络带宽跑满了万兆交换机的极限.</p><h3 id=跨主机物理机到dockerhost模式>跨主机物理机到Docker(host模式)</h3><p>在Docker中运行<code>iperf3</code>相当简单，在<code>hub.docker.com</code>可以找到大量的打包有iperf3的镜像，我们采用:</p><pre><code># sudo docker pull networkstatic/iperf3
</code></pre><p>在服务器端启动侦听<code>5203</code>端口的docker实例:</p><pre><code>[root@192.192.192.89 ~]# docker run --net=host  -it --rm --name=iperf3-server networkstatic/iperf3 -s -p 5203
</code></pre><p>在Client端执行对应的修改，得到的结果为:</p><pre><code>[root@192.192.192.88 ~]# iperf3 -c 192.192.192.89 -p 5203
Connecting to host 192.192.192.89, port 5203
[  4] local 192.192.192.88 port 40326 connected to 192.192.192.89 port 5203
[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd
[  4]   0.00-1.00   sec  1.10 GBytes  9.43 Gbits/sec   20    625 KBytes       
[  4]   1.00-2.00   sec  1.10 GBytes  9.42 Gbits/sec    0    625 KBytes       
//.....
</code></pre><p>结果差不多相同: 9.40 Gbits/sec</p><h3 id=跨主机物理机到dockerbridge模式>跨主机物理机到Docker(Bridge模式)</h3><p>更改为5204端口，这次使用的网络模式为<code>Bridge</code>模式:</p><pre><code>[root@192.192.192.89 ~]# docker run  -it --rm -p 5204:5204 --name=iperf3-server networkstatic/iperf3 -s -p 5204
</code></pre><p>在客户端不作任何修改，只更换远端端口为5204:</p><pre><code>[root@192.192.192.88 ~]# iperf3 -c 192.192.192.89 -p 5204
Connecting to host 192.192.192.89, port 5204
[  4] local 192.192.192.88 port 53936 connected to 192.192.192.89 port 5204
[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd
[  4]   0.00-1.00   sec  1.10 GBytes  9.44 Gbits/sec   15    669 KBytes       
[  4]   1.00-2.00   sec  1.10 GBytes  9.42 Gbits/sec    0    682 KBytes       
[  4]   2.00-3.00   sec  1.10 GBytes  9.42 Gbits/sec    0    691 KBytes 
</code></pre><p>可以看到，在Bridge模式下，吞吐量也跑满了万兆网络的极限.</p><h3 id=同主机物理机到dockerbridge模式>同主机物理机到Docker(Bridge模式)</h3><p>在同一台主机上(192.192.192.89)上运行iperf，测试到Docker的吞吐量，沿用之前侦听5204的容器不变:</p><pre><code>[root@192.192.192.89 ~]# iperf3 -c 192.192.192.89 -p 5204
Connecting to host 192.192.192.89, port 5204
[  4] local 192.192.192.89 port 46720 connected to 192.192.192.89 port 5204
[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd
[  4]   0.00-1.00   sec  2.77 GBytes  23.8 Gbits/sec    0    274 KBytes       
[  4]   1.00-2.00   sec  2.75 GBytes  23.6 Gbits/sec    0    274 KBytes       
[  4]   2.00-3.00   sec  2.75 GBytes  23.6 Gbits/sec    0    277 KBytes       
</code></pre><p>在这种模式下，网络的吞吐量几乎三倍于万兆网络，这是因为从主机到Docker实例的网络通路会走本地的回环接口(lo-loopback)接口。</p><h3 id=同主机docker到dockerbridge模式-external>同主机Docker到Docker(Bridge模式-external)</h3><p>沿用侦听5204端口的容器不变，新启动一个容器，在其中运行iperf:</p><pre><code># iperf3 -c 192.192.192.89 -p 5204
Connecting to host 192.192.192.89, port 5204
[  4] local 172.17.0.5 port 59574 connected to 192.192.192.89 port 5204
[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd
[  4]   0.00-1.00   sec  1.03 GBytes  8.84 Gbits/sec   91    228 KBytes       
[  4]   1.00-2.00   sec   955 MBytes  8.01 Gbits/sec    0    229 KBytes       
[  4]   2.00-3.00   sec  1.02 GBytes  8.80 Gbits/sec    0    230 KBytes       
[  4]   3.00-4.00   sec   767 MBytes  6.43 Gbits/sec    0    230 KBytes       
[  4]   4.00-5.00   sec   851 MBytes  7.14 Gbits/sec    0    230 KBytes       
</code></pre><p>可以看到，如果直接使用物理机的IP地址和端口，则吞吐需要同时使用Bridge模式下物理网卡的吞吐，
此时网卡的物理性能下降明显。</p><h3 id=同主机docker到dockerbridge模式-internal>同主机Docker到Docker(Bridge模式-internal)</h3><p>为了避免使用物理机的IP地址带来的性能下降，直接使用容器内部的IP地址做iperf测试:</p><pre><code># iperf3 -c 172.17.0.4 -p 5204
Accepted connection from 172.17.0.5, port 39516
[  5] local 172.17.0.4 port 5204 connected to 172.17.0.5 port 39518
[ ID] Interval           Transfer     Bandwidth
[  5]   0.00-1.00   sec  2.39 GBytes  20.5 Gbits/sec                  
[  5]   1.00-2.00   sec  2.50 GBytes  21.5 Gbits/sec                  
[  5]   2.00-3.00   sec  2.50 GBytes  21.5 Gbits/sec 
</code></pre><p>可以看到，在这种模式下，容器之间的通信还是基于lo(loopback)接口来做的，几乎三倍于万兆交换机的峰值速度。</p><h3 id=结论>结论</h3><p>各次测试的对比数据整理如下:</p><pre><code>| 物理机-物理机                           | 9.40 Gbit/sec    | 100%  |
| 跨物理机到Docker(host模式网络)          | 9.40 Gbit/sec    | 100%  |
| 跨物理机到Docker(Bridge模式网络)        | 9.40 Gbit/sec    | 100%  |
| 同主机内到Docker(Bridge模式网络)        | 23.8 Gbit/sec    | 250%  |
| 同主机Docker到Docker(Bridge模式-ex)     | 8.00 Gbit/sec    |  85%  |
| 同主机Docker到Docker(Bridge模式-int)    | 21.00 Gbit/sec   | 220%  |
</code></pre><p>结论： 在Docker运行环境中，网络的吞吐量近似于本地网络IO，基本上不会有性能损耗。需要特别注意的是，一定要避免同主机中的Docker实例彼此使用物理机IP/端口进行通信，那样会带来性能的明显下降。</p></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/79/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/79/>79</a></li><li class="page-item active"><a class=page-link href=/page/80/>80</a></li><li class=page-item><a class=page-link href=/page/81/>81</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/228/>228</a></li><li class=page-item><a href=/page/81/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/228/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>