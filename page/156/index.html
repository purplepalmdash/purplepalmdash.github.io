<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/04/13/an-zhuang-icehouse-at-ubuntu14-dot-04-5/>安装Icehouse@Ubuntu14.04(5)</a></h1><span class=post-date>Apr 13, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=neutron-database>Neutron Database</h3><p>Follow following steps for create the database:</p><pre><code>root@JunoController:~# mysql -u root -p
Enter password: 
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 58
Server version: 5.5.41-MariaDB-1ubuntu0.14.04.1 (Ubuntu)

Copyright (c) 2000, 2014, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE neutron;
Query OK, 1 row affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'xxxxx'
    -&gt; ;
Query OK, 0 rows affected (0.01 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'xxxxx';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; flush privileges;
Query OK, 0 rows affected (0.01 sec)

MariaDB [(none)]&gt; quit
Bye

</code></pre><h3 id=keystone-items>Keystone items</h3><p>创建用户:</p><pre><code>root@JunoController:~# source ~/openstack/admin-openrc.sh
root@JunoController:~# keystone user-create --name neutron --pass xxxxx
+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|  email   |                                  |
| enabled  |               True               |
|    id    | a4cbae42a2164c6e9a4c05c3f6835782 |
|   name   |             neutron              |
| username |             neutron              |
+----------+----------------------------------+

</code></pre><p>更改权限，服务为tenant, 角色是admin:</p><pre><code>root@JunoController:~# keystone user-role-add --user neutron --tenant service --role admin

</code></pre><p>创建服务：</p><pre><code>root@JunoController:~# keystone service-create --name neutron --type network --description &quot;OpenStack Networking&quot;
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |       OpenStack Networking       |
|   enabled   |               True               |
|      id     | 1142b316e4e04061bb676b73d0cf6f68 |
|     name    |             neutron              |
|     type    |             network              |
+-------------+----------------------------------+

</code></pre><p>创建服务的end-point:</p><pre><code>root@JunoController:~# keystone endpoint-create --service-id $(keystone service-list | awk '/ network / {print $2}') --publicurl http://10.17.17.211:9696 --adminurl http://10.17.17.211:9696 --internalurl http://10.17.17.211:9696 --region regionOne
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
|   adminurl  |     http://10.17.17.211:9696     |
|      id     | 77bb946d42dc4d099875ecc377510937 |
| internalurl |     http://10.17.17.211:9696     |
|  publicurl  |     http://10.17.17.211:9696     |
|    region   |            regionOne             |
|  service_id | 1142b316e4e04061bb676b73d0cf6f68 |
+-------------+----------------------------------+

</code></pre><h3 id=安装组件>安装组件</h3><p>在Controller端安装:</p><pre><code>root@JunoController:~#  apt-get -y install neutron-server neutron-plugin-ml2 python-neutronclient

</code></pre><p>取得tenant service id:</p><pre><code>root@JunoController:~# source ~/openstack/admin-openrc.sh
root@JunoController:~# keystone tenant-get service
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |          Service Tenant          |
|   enabled   |               True               |
|      id     | 4b22bf4e6a68419aa91da6e0ffaca2dc |
|     name    |             service              |
+-------------+----------------------------------+

</code></pre><p>编辑nova配置文件，修改如下：</p><pre><code>root@JunoController:~# vim /etc/neutron/neutron.conf
[DEFAULT]
rpc_backend = neutron.openstack.common.rpc.impl_kombu
rabbit_host = 10.17.17.211
rabbit_password = xxxxx

notify_nova_on_port_status_changes = True
notify_nova_on_port_data_changes = True
nova_url = http://10.17.17.211:8774/v2
nova_admin_username = nova
nova_admin_tenant_id = 4b22bf4e6a68419aa91da6e0ffaca2dc
nova_admin_password = xxxxx
nova_admin_auth_url = http://10.17.17.211:35357/v2.0

core_plugin = ml2
service_plugins = router
allow_overlapping_ips = True

auth_strategy = keystone

[keystone_authtoken]
auth_uri = http://10.17.17.211:5000
auth_host = 10.17.17.211
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = neutron
admin_password = xxxxx
signing_dir = $state_path/keystone-signing
[database]
connection = mysql://neutron:xxxxx@10.17.17.211/neutron

</code></pre><p>编辑ML2(Modular Layer2)插件， 在控制节点上：</p><pre><code>root@JunoController:~# vim /etc/neutron/plugins/ml2/ml2_conf.ini | more
[ml2]
type_drivers = gre
tenant_network_types = gre
mechanism_drivers = openvswitch

[ml2_type_gre]
tunnel_id_ranges = 1:1000


[securitygroup]
# Controls if neutron security group is enabled or not.
# It should be false when you use nova security group.
# enable_security_group = True
firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
enable_security_group = True

</code></pre><p>调整Compute Service使用Neutron服务:</p><pre><code># vim /etc/nova/nova.conf
network_api_class = nova.network.neutronv2.api.API
neutron_url = http://10.17.17.211:9696
neutron_auth_strategy = keystone
neutron_admin_tenant_name = service
neutron_admin_username = neutron
neutron_admin_password = xxxxx
neutron_admin_auth_url = http://10.17.17.211:35357/v2.0
linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver
firewall_driver = nova.virt.firewall.NoopFirewallDriver
security_group_api = neutron


</code></pre><p>重启服务:</p><pre><code># service nova-api restart
# service nova-scheduler restart
# service nova-conductor restart

</code></pre><p>重启网络服务:</p><pre><code># service neutron-server restart

</code></pre><p>检查是否完成的命令:</p><pre><code>root@JunoController:~# neutron ext-list
+-----------------------+-----------------------------------------------+
| alias                 | name                                          |
+-----------------------+-----------------------------------------------+
| security-group        | security-group                                |
| l3_agent_scheduler    | L3 Agent Scheduler                            |
| ext-gw-mode           | Neutron L3 Configurable external gateway mode |
| binding               | Port Binding                                  |
| provider              | Provider Network                              |
| agent                 | agent                                         |
| quotas                | Quota management support                      |
| dhcp_agent_scheduler  | DHCP Agent Scheduler                          |
| multi-provider        | Multi Provider Network                        |
| external-net          | Neutron external network                      |
| router                | Neutron L3 Router                             |
| allowed-address-pairs | Allowed Address Pairs                         |
| extra_dhcp_opt        | Neutron Extra DHCP opts                       |
| extraroute            | Neutron Extra Route                           |
+-----------------------+-----------------------------------------------+

</code></pre><p>###配置网络节点
激活以下选项:</p><pre><code>root@JunoNetwork:~# vim /etc/sysctl.conf
net.ipv4.ip_forward=1
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.rp_filter=0
net.bridge.bridge-nf-call-arptables=1
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1

</code></pre><p>提交更改(这里有错误):</p><pre><code>root@JunoNetwork:~# sysctl -p
net.ipv4.ip_forward = 1
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-arptables: No such file or directory
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: No such file or directory

</code></pre><p>安装网络组件:</p><pre><code>root@JunoNetwork:~# apt-get install neutron-plugin-ml2 neutron-plugin-openvswitch-agent neutron-l3-agent neutron-dhcp-agent

</code></pre><p>配置通用组件:</p><pre><code># vim /etc/neutron/neutron.conf
[DEFAULT]
rpc_backend = neutron.openstack.common.rpc.impl_kombu
rabbit_host = 10.17.17.211
rabbit_password = xxxxx
core_plugin = ml2
service_plugins = router
allow_overlapping_ips = True
verbose = True
auth_strategy = keystone

[keystone_authtoken]
auth_uri = http://10.17.17.211:5000
auth_host = 10.17.17.211
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = neutron
admin_password = xxxxx

</code></pre><p>编辑L3 agent:</p><pre><code># vim /etc/neutron/l3_agent.ini
[DEFAULT]
interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver
use_namespaces = True
verbose = True

</code></pre><p>编辑DHCP插件:</p><pre><code># vim /etc/neutron/dhcp_agent.ini
 [DEFAULT]
interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
use_namespaces = True

</code></pre><p>配置DHCP：</p><pre><code>root@JunoNetwork:~# vim /etc/neutron/dhcp_agent.ini
[DEFAULT]
...
dnsmasq_config_file = /etc/neutron/dnsmasq-neutron.conf
root@JunoNetwork:~# vim /etc/neutron/dnsmasp-neutron.conf
dhcp-option-force=26,1454

</code></pre><p>配置metadata agent:</p><pre><code>root@JunoNetwork:~# vim /etc/neutron/metadata_agent.ini
[DEFAULT]
auth_url = http://10.17.17.211:5000/v2.0
auth_region = regionOne
admin_tenant_name = service
admin_user = neutron
admin_password = xxxxx
nova_metadata_ip = 10.17.17.211
metadata_proxy_shared_secret = xxxxx

</code></pre><p>回到controller节点，编辑:</p><pre><code># vim /etc/nova/nova.conf
[DEFAULT]
...
service_neutron_metadata_proxy = true
neutron_metadata_proxy_shared_secret = xxxxx

</code></pre><p>重启compute api服务:</p><pre><code># service nva-api restart

</code></pre><p>配置 ml2:</p><pre><code>root@JunoNetwork:~# vim /etc/neutron/plugins/ml2/ml2_conf.ini 
[ml2]
type_drivers = gre
tenant_network_types = gre
mechanism_drivers = openvswitch
[ml2_type_gre]
tunnel_id_ranges = 1:1000
[ovs]
local_ip = 10.19.19.212
tunnel_type = gre
enable_tunneling = True
[securitygroup]
firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
enable_security_group = True

</code></pre><p>重启openvswitch 服务:</p><pre><code>root@JunoNetwork:~# service openvswitch-switch restart

</code></pre><p>增加bridge配置：</p><pre><code>root@JunoNetwork:~# ovs-vsctl add-br br-ex
root@JunoNetwork:~# cat /etc/network/interfaces
auto eth2
iface eth2 inet manual

iface br-ex inet static
address 10.22.22.212
netmask 255.255.255.0
gateway 10.22.22.1
bridge_ports eth2
bridge_stp off
auto br-ex

</code></pre><p>增加桥接端口，并且重启机器:</p><pre><code>root@JunoNetwork:~# ovs-vsctl add-port br-ex eth2
root@JunoNetwork:~# reboot

</code></pre><p>###计算节点配置
更改sysctl配置:</p><pre><code>root@JunoCompute:~# vim /etc/sysctl.conf
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.rp_filter=0
net.bridge.bridge-nf-call-arptables=1
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
root@JunoCompute:~# sysctl -p 
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1

</code></pre><p>安装下列包:</p><pre><code># apt-get install neutron-common neutron-plugin-ml2 neutron-plugin-openvswitch-agent openvswitch-datapath-dkms

</code></pre><p>配置compute节点上的网络通用组件:</p><pre><code>root@JunoCompute:~# vim /etc/neutron/neutron.conf
    [DEFAULT]
    auth_strategy = keystone
    rpc_backend = neutron.openstack.common.rpc.impl_kombu
    rabbit_host = controller
    rabbit_password = xxxx
    core_plugin = ml2
    service_plugins = router
    allow_overlapping_ips = True
    verbose = True
    [keystone_authtoken]
    auth_uri = http://10.17.17.211:5000
    auth_host = 10.17.17.211
    auth_port = 35357
    auth_protocol = http
    admin_tenant_name = service
    admin_user = neutron
    admin_password = xxxx
    signing_dir = $state_path/keystone-signing
    [database]
root@JunoCompute:~# vim /etc/neutron/plugins/ml2/ml2_conf.ini 
    [DEFAULT]
    ...
    core_plugin = ml2
    service_plugins = router
    allow_overlapping_ips = True
    [ml2]
    ...
    type_drivers = gre
    tenant_network_types = gre
    mechanism_drivers = openvswitch
    [ml2_type_gre]
    ...
    tunnel_id_ranges = 1:1000
    [ovs]
    ...
    local_ip = 10.19.19.213
    tunnel_type = gre
    enable_tunneling = True
    [securitygroup]
    ...
    firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
    enable_security_group = True
root@JunoCompute:~# service openvswitch-switch restart
root@JunoCompute:~# service nova-compute restart
root@JunoCompute:~# service neutron-plugin-openvswitch-agent restart

</code></pre><p>接下来我们配置Compute节点上的nova,让它使用neutron作为网络管理器.</p><pre><code>root@JunoCompute:~# vim /etc/nova/nova.conf 
[DEFAULT]
network_api_class = nova.network.neutronv2.api.API
neutron_url = http://10.17.17.211:9696
neutron_auth_strategy = keystone
neutron_admin_tenant_name = service
neutron_admin_username = neutron
neutron_admin_password = xxxx
neutron_admin_auth_url = http://10.17.17.211:35357/v2.0
linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver
firewall_driver = nova.virt.firewall.NoopFirewallDriver
security_group_api = neutron

</code></pre><p>修改完毕后，重启Compute节点上的服务:</p><pre><code>root@JunoCompute:~# service nova-compute restart
nova-compute stop/waiting
nova-compute start/running, process 2266
root@JunoCompute:~# service neutron-plugin-openvswitch-agent restart
stop: Unknown instance: 
neutron-plugin-openvswitch-agent start/running, process 2303

</code></pre><p>配置Network节点的网络，因为我们需要br-ex作为对外网络的接口。<br>配置网络如下：</p><pre><code># ovs-vsctl add-br br-ex
# ovs-vsctl add-port br-ex eth2
# cat /etc/network/interfaces
# 
auto eth2
iface eth2 inet manual

iface br-ex inet static
address 10.22.22.212
netmask 255.255.255.0
gateway 10.22.22.1
bridge_ports eth2
bridge_stp off
auto br-ex
# reboot

</code></pre><p>增加ext-net:</p><pre><code>root@JunoController:~# neutron net-create ext-net --shared --router:external=True
Created a new network:
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| id                        | d879d5b1-f16e-4e28-beda-eb2b433e1f39 |
| name                      | ext-net                              |
| provider:network_type     | gre                                  |
| provider:physical_network |                                      |
| provider:segmentation_id  | 1                                    |
| router:external           | True                                 |
| shared                    | True                                 |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tenant_id                 | ea1f0a6b15dc4796958f087c38756ed1     |
+---------------------------+--------------------------------------+

</code></pre><p>外部子网：</p><pre><code>root@JunoController:~# neutron subnet-create ext-net --name ext-subnet --allocation-pool start=10.22.22.10,end=10.22.22.50 --disable-dhcp --gateway 10.22.22.1 --gateway 10.22.22.1 10.22.22.10/24
Created a new subnet:
+------------------+------------------------------------------------+
| Field            | Value                                          |
+------------------+------------------------------------------------+
| allocation_pools | {&quot;start&quot;: &quot;10.22.22.10&quot;, &quot;end&quot;: &quot;10.22.22.50&quot;} |
| cidr             | 10.22.22.0/24                                  |
| dns_nameservers  |                                                |
| enable_dhcp      | False                                          |
| gateway_ip       | 10.22.22.1                                     |
| host_routes      |                                                |
| id               | 3c7e2224-0979-4eb6-b95f-16401ecbfef0           |
| ip_version       | 4                                              |
| name             | ext-subnet                                     |
| network_id       | d879d5b1-f16e-4e28-beda-eb2b433e1f39           |
| tenant_id        | ea1f0a6b15dc4796958f087c38756ed1               |
+------------------+------------------------------------------------+


</code></pre><pre><code>root@JunoController:~# source openstack/demo-openrc.sh 
root@JunoController:~# neutron net-create demo-net
Created a new network:
+----------------+--------------------------------------+
| Field          | Value                                |
+----------------+--------------------------------------+
| admin_state_up | True                                 |
| id             | 01c966ce-88cf-43a2-a7b7-2ebf6d6b6d60 |
| name           | demo-net                             |
| shared         | False                                |
| status         | ACTIVE                               |
| subnets        |                                      |
| tenant_id      | 2ac9cae777014d3d94458f521b013e94     |
+----------------+--------------------------------------+
root@JunoController:~# neutron subnet-create demo-net --name demo-subnet --gateway 10.44.44.1 10.44.44.0/24
Created a new subnet:
+------------------+------------------------------------------------+
| Field            | Value                                          |
+------------------+------------------------------------------------+
| allocation_pools | {&quot;start&quot;: &quot;10.44.44.2&quot;, &quot;end&quot;: &quot;10.44.44.254&quot;} |
| cidr             | 10.44.44.0/24                                  |
| dns_nameservers  |                                                |
| enable_dhcp      | True                                           |
| gateway_ip       | 10.44.44.1                                     |
| host_routes      |                                                |
| id               | c6181123-f729-4ad2-bddc-93cfc761d0e1           |
| ip_version       | 4                                              |
| name             | demo-subnet                                    |
| network_id       | 01c966ce-88cf-43a2-a7b7-2ebf6d6b6d60           |
| tenant_id        | 2ac9cae777014d3d94458f521b013e94               |
+------------------+------------------------------------------------+

root@JunoController:~# neutron router-create demo-router
Created a new router:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| external_gateway_info |                                      |
| id                    | e5a010ba-371c-43d2-b3fb-a30e0dc5302b |
| name                  | demo-router                          |
| status                | ACTIVE                               |
| tenant_id             | 2ac9cae777014d3d94458f521b013e94     |
+-----------------------+--------------------------------------+

root@JunoController:~# neutron router-interface-add demo-router demo-subnet
Added interface c862f772-a1ef-4401-9a3b-2bdf5444e41b to router demo-router.

root@JunoController:~# neutron router-gateway-set demo-router ext-net
Set gateway for router demo-router

</code></pre><p>检查，在外网上ping 10.22.22.10这个地址，因为路由器占用了一个地址，所以如果能ping通这个地址，说明我们创建的网络是好的。</p><pre><code>[root:~]# ping 10.22.22.212                                                                                                                    
PING 10.22.22.212 (10.22.22.212) 56(84) bytes of data.                                                                                         
64 bytes from 10.22.22.212: icmp_seq=1 ttl=64 time=0.152 ms                                                                                    
64 bytes from 10.22.22.212: icmp_seq=2 ttl=64 time=0.136 ms    

</code></pre><p>检查agent状态:</p><pre><code>root@JunoController:~# neutron agent-list
+--------------------------------------+--------------------+-------------+-------+----------------+
| id                                   | agent_type         | host        | alive | admin_state_up |
+--------------------------------------+--------------------+-------------+-------+----------------+
| 0b7191e1-ecd2-4808-b87a-f616d0a3bc7b | Metadata agent     | JunoNetwork | :-)   | True           |
| 34511134-8392-44a9-a889-0ff03d85a995 | Open vSwitch agent | JunoCompute | :-)   | True           |
| 474065d1-a50a-4d11-89d3-37c7a88e449c | DHCP agent         | JunoNetwork | :-)   | True           |
| 5569c590-df83-4ee1-a073-15c908ef8d20 | L3 agent           | JunoNetwork | :-)   | True           |
| a22c6e2a-7af0-4404-9e5b-46996b370672 | Open vSwitch agent | JunoNetwork | :-)   | True           |
+--------------------------------------+--------------------+-------------+-------+----------------+

</code></pre><p>在 Compute Node 上的 OVS agent出现后，才能代表我们的网络配置成功。</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/04/07/deploy-opencontrail-on-centos-with-docker/>Deploy OpenContrail On CentOS With Docker As Hypervisor</a></h1><span class=post-date>Apr 7, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>Reference:<br><a href=https://software.intel.com/en-us/blogs/2014/12/28/experimenting-with-openstack-sahara-on-docker-containers>https://software.intel.com/en-us/blogs/2014/12/28/experimenting-with-openstack-sahara-on-docker-containers</a><br>I wanna enable the docker as hypervisor then it would greatly save the resources, and benefit with docker&rsquo;s rich resources. Following is the steps:</p><h3 id=preparation>Preparation</h3><p>First create the image file via:</p><pre><code># qemu-img create -f qcow2 CentOSOpenContrail.qcow2 100G
Formatting 'CentOSOpenContrail.qcow2', fmt=qcow2 size=107374182400 encryption=off cluster_size=65536 
[root:/home/juju/img/CentOSOpenContrail]# pwd
/home/juju/img/CentOSOpenContrail

</code></pre><p>Then create a virtual machine based on KVM, allocate 8G Memory, 4-core, which copies the host CPU configuration.</p><h3 id=installation>Installation</h3><p>After installation, update the installed software via:</p><pre><code>$ mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup   
$ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 
$ sudo yum makecache
$ sudo yum update -y
$ sudo reboot

</code></pre><p>Install following packages:</p><pre><code># wget https://repos.fedorapeople.org/repos/openstack/openstack-juno/rdo-release-juno-1.noarch.rpm
# rpm -ivh rdo-release-juno-1.noarch.rpm 
# yum install –y https://rdo.fedorapeople.org/rdo-release.rpm
# yum install openstack-packstack

</code></pre><p>Now you could use packstack for installing the packages:</p><pre><code># packstack --gen-answer-file=/root/answer.txt
# packstack --answer-file=/root/answer.txt

</code></pre><p>Install openstack-sahara via following command, the python-tox enable tox for generating the configuration files for sahara:</p><pre><code># yum install openstack-sahara 
# yum install python-tox

</code></pre><p>Create the username and password for sahara to use:</p><pre><code>[root@10-17-17-183 etc]# mysql
MariaDB [(none)]&gt; create user 'sahara'@'localhost' identified by 'saharapass';
Query OK, 0 rows affected (0.07 sec)
MariaDB [(none)]&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| cinder             |
| glance             |
| keystone           |
| mysql              |
| neutron            |
| nova               |
| performance_schema |
| test               |
+--------------------+
9 rows in set (0.00 sec)

MariaDB [(none)]&gt; use mysql
MariaDB [mysql]&gt; show tables;
MariaDB [mysql]&gt; select * from user;
+-----------+----------------+-----------------------------------

</code></pre><p>How to get all of the password configuration information of packstack?</p><pre><code>$ cd /etc/
$ grep -i &quot;sql://&quot; ./ -r

</code></pre><p>Create a database named &lsquo;saharaDB&rdquo; and grant it to user sahra:</p><pre><code>MariaDB [(none)]&gt; create database saharaDB;
MariaDB [(none)]&gt; grant all on saharaDB.* to 'sahara'@'localhost';
Query OK, 0 rows affected (0.02 sec)

MariaDB [(none)]&gt; quit;
[root@10-17-17-183 etc]# mysql -h 127.0.0.1 -u sahara -p saharaDB
Enter password: 
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 1481
Server version: 5.5.40-MariaDB-wsrep MariaDB Server, wsrep_25.11.r4026

Copyright (c) 2000, 2014, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [saharaDB]&gt; 

</code></pre><p>So the syntax for sahara to use is like:</p><pre><code># cat /etc/sahara/sahara.conf
connection = mysql://sahara:saharapass@127.0.0.1/saharaDB
use_neutron=true

# sahara-db-manage --config-file /etc/sahara/sahara.conf upgrade head
INFO  [alembic.migration] Context impl MySQLImpl.
INFO  [alembic.migration] Will assume non-transactional DDL.
INFO  [alembic.migration] Running upgrade None -&gt; 001, Icehouse release
INFO  [alembic.migration] Running upgrade 001 -&gt; 002, placeholder
INFO  [alembic.migration] Running upgrade 002 -&gt; 003, placeholder
INFO  [alembic.migration] Running upgrade 003 -&gt; 004, placeholder
INFO  [alembic.migration] Running upgrade 004 -&gt; 005, placeholder
INFO  [alembic.migration] Running upgrade 005 -&gt; 006, placeholder
INFO  [alembic.migration] Running upgrade 006 -&gt; 007, convert clusters.status_description to LongText
INFO  [alembic.migration] Running upgrade 007 -&gt; 008, add security_groups field to node groups
INFO  [alembic.migration] Running upgrade 008 -&gt; 009, add rollback info to cluster
INFO  [alembic.migration] Running upgrade 009 -&gt; 010, add auto_security_groups flag to node group
INFO  [alembic.migration] Running upgrade 010 -&gt; 011, add Sahara settings info to cluster

</code></pre><p>REgister the service and specify the endpoint:</p><pre><code>[root@10-17-17-183 etc]# cat ./nagios/keystonerc_admin                                                                                         
export OS_USERNAME=admin                                                                                                                       
export OS_TENANT_NAME=admin                                                                                                                    
export OS_PASSWORD=5d4e62e79d314477                                                                                                            
export OS_AUTH_URL=http://10.17.17.183:35357/v2.0/ 
[root@10-17-17-183 etc]# pwd                                                                
/etc  
# keystone service-create --name sahara --type data_processing --description &quot;Data processing service&quot;
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |     Data processing service      |
|   enabled   |               True               |
|      id     | 5f711f0d42754b349931349bd0c325d1 |
|     name    |              sahara              |
|     type    |         data_processing          |
+-------------+----------------------------------+
[root@10-17-17-183 ~]# keystone endpoint-create --service-id $(keystone service-list | awk '/ sahara / {print $2}') --publicurl http://127.0.0.1:8386/v1.1/%\(tenant_id\)s --internalurl http://127.0.0.1:8386/v1.1/%\(tenant_id\)s --adminurl http://127.0.0.1:8386/v1.1/%\(tenant_id\)s --region regionOne
+-------------+------------------------------------------+
|   Property  |                  Value                   |
+-------------+------------------------------------------+
|   adminurl  | http://127.0.0.1:8386/v1.1/%(tenant_id)s |
|      id     |     b2115bab8bd743f589b1ed68eef69b4c     |
| internalurl | http://127.0.0.1:8386/v1.1/%(tenant_id)s |
|  publicurl  | http://127.0.0.1:8386/v1.1/%(tenant_id)s |
|    region   |                regionOne                 |
|  service_id |     5f711f0d42754b349931349bd0c325d1     |
+-------------+------------------------------------------+
[root@10-17-17-183 ~]# systemctl start openstack-sahara-all
[root@10-17-17-183 ~]# systemctl enable openstack-sahara-all
ln -s '/usr/lib/systemd/system/openstack-sahara-all.service' '/etc/systemd/system/multi-user.target.wants/openstack-sahara-all.service'
[root@10-17-17-183 ~]# systemctl status openstack-sahara-all.service

</code></pre><p>More detailed info could be fetched from:<br><a href=http://docs.openstack.org/juno/install-guide/install/apt/content/sahara-install.html>http://docs.openstack.org/juno/install-guide/install/apt/content/sahara-install.html</a><br>Now login to the <a href=http://10.17.17.183>http://10.17.17.183</a> then you could visit the Trustyboard. The password is the one we used in the <code>OS_PASSWORD</code>.<br>Install docker:</p><pre><code>$  yum install docker
Or
$  wget https://get.docker.com/builds/Linux/x86_64/docker-latest -O docker
$  docker --version
[root@10-17-17-183 ~]#  usermod -G dockerroot nova
[root@10-17-17-183 ~]# service openstack-nova-compute restart
Redirecting to /bin/systemctl restart  openstack-nova-compute.service
# systemctl restart docker
# systemctl enable docker

</code></pre><p>Install nova-docker support:</p><pre><code>$  yum install python-pip
$  pip install -e git+https://github.com/stackforge/nova-docker#egg=novadocker
$  cd src/novadocker/
$  python setup.py install
$ vim /etc/nova/nova.conf
[DEFAULT]
compute_driver = novadocker.virt.docker.DockerDriver

</code></pre><p>Edit the nova&rsquo;s rootwrap like following:</p><pre><code>[root@10-17-17-183 nova]# mkdir rootwrap.d
[root@10-17-17-183 nova]# cd rootwrap.d/
[root@10-17-17-183 rootwrap.d]# touch docker.filters
[root@10-17-17-183 rootwrap.d]# vim docker.filters 
[Filters]
# nova/virt/docker/driver.py:'ln', '-sf', '/var/run/netns/.*'
ln: CommandFilter, /bin/ln, root
[root@10-17-17-183 rootwrap.d]# vim /etc/nova/rootwrap.conf 
[root@10-17-17-183 rootwrap.d]# pwd
/etc/nova/rootwrap.d

</code></pre><p>Edit the glance-api.conf</p><pre><code>[root@10-17-17-183 glance]# vim ./glance-api.conf 
[DEFAULT]
container_formats = ami,ari,aki,bare,ovf,docker
[root@10-17-17-183 glance]# pwd
/etc/glance

</code></pre><p>Create the ubuntu Container images via following commands:</p><pre><code>$ docker pull ubuntu
$ docker save ubuntu | glance image-create --is-public=True --container-format=docker --disk-format=raw --name ubuntu_container

</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/04/01/build-qemu-for-supporting-glustfs/>Build qemu for supporting glustfs</a></h1><span class=post-date>Apr 1, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>Following is the build procedure.</p><pre><code>$ sudo apt-get build-dep qemu
$ sudo apt-get install libvde-dev libvdeplug2-dev libcap-ng-dev libattr1-dev
$ wget http://wiki.qemu-project.org/download/qemu-2.0.2.tar.bz2
$ tar xjvf qemu-2.0.2.tar.bz2
$ cd qemu-2.0.2/
$ mkdir -p bin/debug/native
$ cd bin/debug/native
$ sudo apt-get install libjpeg-turbo8-dev
$ sudo apt-get install glusterfs-common
 ../../../configure --enable-sdl --audio-drv-list=alsa,oss --enable-curses --enable-vnc-jpeg --enable-curl --enable-fdt --enable-kvm --enable-tcg-interpreter --enable-system --enable-user \\n --enable-linux-user --enable-guest-base --enable-pie --enable-uuid --enable-vde --enable-linux-aio --enable-cap-ng --enable-attr --enable-docs --enable-vhost-net --enable-rbd \\n --enable-guest-agent --enable-glusterfs --target-list=x86_64-softmmu,i386-softmmu
 ./qemu-img -h
 $ make -j2

</code></pre><p>Before, Found qemu-img:</p><pre><code>$ qemu-img -h
Supported formats: vvfat vpc vmdk vhdx vdi sheepdog sheepdog sheepdog rbd raw host_cdrom host_floppy host_device file qed qcow2 qcow parallels nbd nbd nbd dmg tftp ftps ftp https http cow cloop bochs blkverify blkdebug

</code></pre><p>After, qemu-img:</p><pre><code>Supported formats: vvfat vpc vmdk vhdx vdi sheepdog sheepdog sheepdog rbd raw host_cdrom host_floppy host_device file qed qcow2 qcow parallels nbd nbd nbd gluster gluster gluster gluster dmg tftp ftps ftp https http cow cloop bochs blkverify blkdebug

</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/03/30/whole-process-for-deploying-contrail/>Whole Process For Deploying Contrail</a></h1><span class=post-date>Mar 30, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>Following are the steps, enjoy them:</p><pre><code># First bootstrap the environment.   
juju bootstrap --metadata-source ~/.juju/metadata --upload-tools -v --show-log --constraints=&quot;mem=3G&quot;

#####################################################
# Machine 0, hold 9 services. 3G mem. trusty
# Memory: 3G
# Service: 10
#####################################################


# Since machine 0 is ready for using, deploy services to this node via following commands:     
# Juju-gui is for monitoring the status and the components
# 1. juju-gui
juju deploy --to 0 --repository=/home/Trusty/charms/ local:trusty/juju-gui
# 2. rabbitmq-server
juju deploy --to lxc:0 --repository=/home/Trusty/charms/ local:trusty/rabbitmq-server
# 3. mysql
juju deploy --to lxc:0 --repository=/home/Trusty/charms/ local:trusty/mysql
# 4. keystone
juju deploy --to lxc:0 --repository=/home/Trusty/Code/deployOpenContrail/contrail-deployer/src/charms/ local:trusty/keystone
# 5. openstack-Trustyboard
juju deploy --to lxc:0 --repository=/home/Trusty/Code/deployOpenContrail/contrail-deployer/src/charms/ local:trusty/openstack-Trustyboard

# 6. nova-cloud-controller
juju deploy --to lxc:0 --repository=/home/Trusty/Code/deployOpenContrail/contrail-deployer/src/charms/ local:trusty/nova-cloud-controller --config=/home/Trusty/Code/deploy/nova-cloud-controller-config.yaml 
# 7. glance to lxc:0
juju deploy --to lxc:0 --repository=/home/Trusty/Code/deployOpenContrail/contrail-deployer/src/charms/ local:trusty/glance
# 8. contrail-configuration to lxc:0
juju deploy --to lxc:0 --repository=/home/Trusty/Code/deployOpenContrail/contrail-deployer/src/charms/ local:trusty/contrail-configuration
# 9. control-control to lxc:0
juju deploy --to lxc:0 --repository=/home/Trusty/Code/deployOpenContrail/contrail-deployer/src/charms/ local:trusty/contrail-control
# 10. neutron-api to lxc:0
juju deploy --to lxc:0 --repository=/home/Trusty/Code/deployOpenContrail/contrail-deployer/src/charms/ local:trusty/neutron-api --config=/home/Trusty/Code/deploy/neutron-api.yaml

# Now add the first node for nova-compute.   
juju set-constraints &quot;mem=900M&quot;
#####################################################
# Machine 1, hold nova-compute service, 2G with nested CPU, trusty
# Memory: 2G
# Service: 1
#####################################################

# Add name specified machine, which memory is 2048M, with cpu nested for kvm
juju add-machine MaasOpenContrail4
# Machine 1 will be added into the Juju environment, deploy nova-compute into this node
juju deploy --to 1 --repository=/home/Trusty/Code/deployOpenContrail/contrail-deployer/src/charms/ local:trusty/nova-compute

#####################################################
# Machine 2, hold neutron-gateway service, 1G , trusty
# Memory: 1G
# Service: 1
#####################################################
# Add neutron-gateway to 1G based machine , it may fail, so you could remove-machine, and re-add again. via `juju destroy-machine 2 --force`  
juju add-machine MaasOpenContrail7
juju deploy --to 4 --repository=/home/Trusty/Code/deployOpenContrail/contrail-deployer/src/charms/ local:trusty/quantum-gateway neutron-gateway

#####################################################
# Machine 3, hold cassandra and zookeeper serivce, 4G, precise.
# Memory: 4G
# Service: 2
#####################################################
# First we should change the default deployed system version to precise. 
juju set-environment default-series=precise
# Add new machine, whose memory is 4G.
juju add-machine MaasOpenContrail5
# Deploy cassandra to 4G Node
juju deploy --to 5 --repository=/home/Trusty/Code/deployOpenContrail/contrail-deployer/src/charms/ local:precise/cassandra --config=/home/Trusty/Code/deploy/cassandra.yaml
# Also use this machine for deploy a lxc based service, zookeeper, which is also based on precise.
juju deploy --to lxc:5 --repository=/home/Trusty/Code/deployOpenContrail/contrail-deployer/src/charms/ local:precise/zookeeper


# Well, set the default environment from precise to trusty
juju set-environment default-series=trusty

# Finally deploy  neutron-contrail
juju deploy --repository=/home/Trusty/Code/deployOpenContrail/contrail-deployer/src/charms/ local:trusty/neutron-contrail



#####################################################
#####################################################
# Well the deploy is finished, then add relationship
#####################################################
#####################################################
juju add-relation keystone mysql
juju add-relation nova-cloud-controller mysql
juju add-relation nova-cloud-controller rabbitmq-server
juju add-relation nova-cloud-controller glance
juju add-relation nova-cloud-controller keystone
juju add-relation neutron-gateway mysql
juju add-relation neutron-gateway:amqp rabbitmq-server:amqp
juju add-relation neutron-gateway nova-cloud-controller
juju add-relation nova-compute:shared-db mysql:shared-db
juju add-relation nova-compute:amqp rabbitmq-server:amqp
juju add-relation nova-compute glance
juju add-relation nova-compute nova-cloud-controller
juju add-relation glance mysql
juju add-relation glance keystone
juju add-relation openstack-Trustyboard keystone
juju add-relation neutron-api mysql
juju add-relation neutron-api rabbitmq-server
juju add-relation neutron-api nova-cloud-controller
juju add-relation neutron-api:identity-service keystone:identity-service
juju add-relation neutron-api:identity-admin keystone:identity-admin
juju add-relation contrail-configuration:cassandra cassandra:database
juju add-relation contrail-configuration zookeeper
juju add-relation contrail-configuration rabbitmq-server
juju add-relation contrail-configuration keystone
juju add-relation contrail-configuration neutron-gateway
juju add-relation neutron-api contrail-configuration
juju add-relation contrail-control contrail-configuration
juju add-relation nova-compute neutron-contrail
juju add-relation neutron-contrail contrail-control
juju add-relation neutron-contrail neutron-gateway
juju add-relation neutron-contrail contrail-configuration
juju add-relation neutron-contrail keystone


# change the passwd of OpenStack:    
juju set keystone admin-password=&quot;helloworld&quot;

</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/03/27/add-sbh-bluetooth-to-linux/>Add SBH BlueTooth to Linux</a></h1><span class=post-date>Mar 27, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>I bought a SBH Sony Bluetooth headset, following is the steps for adding it to system.</p><h3 id=ubuntu-installation>Ubuntu Installation</h3><p>Install bluetooth related software:</p><pre><code>$ sudo apt-get install -y blueman-manager bluetooth
$ vim ~/.config/awesome/rc.lua
autorunApps =
{
--.........
&quot;blueman-manager&quot;,
&quot;fcitx&quot;,
</code></pre><h3 id=add-device>Add Device</h3><p>Use Blueman for adding the equiment, first click the SBH headset to let it enter discover mode, also in blueman we enable the discover mode too, when setup the equipment, the code you have to enter is 0000.<br>The device kindle is not headset, but the a2dp?</p><h3 id=add-configuration-for-alsa>Add Configuration for ALSA</h3><p>Edit the /etc/asound.conf, my configuration file is listed as following:</p><pre><code>pcm.btheadset {
   type plug
   slave {
       pcm {
           type bluetooth
           device xxx.xxx.xxx.xxx
           profile &quot;auto&quot;
       }   
   }   
   hint {
       show on
       description &quot;BT Headset&quot;
   }   
}
ctl.btheadset {
  type bluetooth
}  
pcm.sbhbtheadset {
   type plug
   slave {
       pcm {
           type bluetooth
           device xxx.xxx.xxx.xxx
           profile &quot;auto&quot;
       }   
   }   
   hint {
       show on
       description &quot;SBH BT Headset&quot;
   }   
}
ctl.sbhbtheadset {
  type bluetooth
}  

</code></pre><p>The first equipment is another bluetooth headset. We define our SDH to sdhbtheadset.<br>Examine our added equipment via following command:</p><pre><code>$ aplay -L
default
    Playback/recording through the PulseAudio sound server
null
    Discard all samples (playback) or generate zero samples (capture)
pulse
    PulseAudio Sound Server
btheadset
    BT Headset
sbhbtheadset
    SBH BT Headset

</code></pre><h3 id=send-sound>Send Sound</h3><p>The commands are listed as following:</p><pre><code>$ pactl load-module module-alsa-sink device=sbhbtheadset
17
[Trusty@~]$ pacmd load_module module-bluetooth-discover
Welcome to PulseAudio! Use &quot;help&quot; for usage information.
&gt;&gt;&gt; Unknown command: load_module module-bluetooth-discover
&gt;&gt;&gt; %                                                             
[Trusty@~]$ pactl list sinks short
1       alsa_output.sbhbtheadset        module-alsa-sink.c      s16le 2ch 44100Hz      SUSPENDED
[Trusty@~]$ pacmd set-default-sink 1
Welcome to PulseAudio! Use &quot;help&quot; for usage information.
&gt;&gt;&gt; &gt;&gt;&gt; %                  

</code></pre><p>Now you are ready for listening Music via bluetooth headset, enjoy it.</p></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/155/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/155/>155</a></li><li class="page-item active"><a class=page-link href=/page/156/>156</a></li><li class=page-item><a class=page-link href=/page/157/>157</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/242/>242</a></li><li class=page-item><a href=/page/157/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/242/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>