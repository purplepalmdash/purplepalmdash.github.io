<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/05/26/chef-setup/>Chef Setup</a></h1><span class=post-date>May 26, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>For automatically deploying OpenStack, I use Chef for deployment, following records the steps for setting up the whole environment.</p><h3 id=machine-preparation>Machine Preparation</h3><p>Chef Server: 2-Core, 3G Memory, IP address: xxx.xxx.10.211, Ubuntu14.04.<br>Chef Workstation: 4-Core, 8G Memory, a physical machine, IP address: xxx.xxx.0.119, Ubuntu14.04.</p><h3 id=install-server>Install Server</h3><p>Install the chef-server package, which downloaded from chef.io website, after installation, simply reconfigure it, this finishes the installation and configuration.</p><pre><code>$ sudo dpkg -i chef-server-core_12.0.8-1_amd64.deb
$ sudo chef-server-ctl reconfigure
</code></pre><p>Configure the permit file, also create the user and organization for the chef:</p><pre><code># sudo chef-server-ctl user-crate YourName FirstName LastName Email PassWord --filename YourPermitFileName
$ sudo chef-server-ctl user-create youname YYYXXX Man xxxxxxx@163.com YOURPASSWORD --filename ~/youname.pem
# sudo chef-server-ctl org-create YourOrgName Your Company Name  --association_user YourUser --filename  YourOrgnizationPermitFile
$ sudo chef-server-ctl org-create youname YYYXXX Software, Inc. --association_user youname --filename ~/youname_org.pem
</code></pre><p>Install opscode-manager and reconfigure it via following commands:</p><pre><code>$ sudo dpkg -i opscode-manage_1.13.0-1_amd64.deb 
$ sudo opscode-manage-ctl reconfigure
</code></pre><p>Now visit the webiste to see the Chef Server UI.</p><p><a href=https://YourURL>https://YourURL</a></p><p><img src=/images/2015_05_26_16_44_58_610x297.jpg alt=/images/2015_05_26_16_44_58_610x297.jpg></p><h3 id=chef-workstation>Chef Workstation</h3><p>I use the physical machine for Chef Workstation.</p><p>Install it via:</p><pre><code>$ sudo dpkg -i chef_12.3.0-1_amd64.deb
</code></pre><p>Fetch back the chef repository from github, configure it and add the ignore directory:</p><pre><code>$ git clone https://github.com/opscode/chef-repo.git
$ cd chef-repo 
$ mkdir .chef
$ echo &quot;.chef&quot;&gt;&gt;~/chef-repo/.gitignore
$ git add .
$ git commit -m &quot;Exclude the ./.chef directory from version control&quot;
[master 64515ff] Exclude the ./.chef directory from version control
 1 file changed, 1 insertion(+)
</code></pre><p>Install the chefdk, and verify the chef, you should see all of the components OK, then you could continue for next step:</p><pre><code>$ sudo dpkg -i chefdk_0.6.0-1_amd64.deb 
$ chef verify
</code></pre><p>Transfer all of the pem file from the ChefServer to ChefWorkstation, and put them under the folder of ~/chef-repo/.chef:</p><pre><code>$ scp xxx@xxxxx:/home/xxx/*.pem xxxx@ChefWorkstation:/home/xxxx/chef-repo/.chef
</code></pre><p>Add following item under the Workstation&rsquo;s configuration:</p><pre><code>$ sudo vim /etc/hosts
XXX.xxx.xxx.xxx  ChefServer
</code></pre><p>Now configure the knife.rb and let your authentification be verified.</p><pre><code>$ vim ~/chef-repo/.chef/knife.rb
current_dir = File.dirname(__FILE__)
log_level                :info
log_location             STDOUT
node_name                &quot;xxxxxxxxx&quot;
client_key               &quot;#{current_dir}/xxxxxxxxx.pem&quot;
validation_client_name   &quot;xxxxxxxxx_org&quot;
validation_key           &quot;#{current_dir}/xxxxxxxxx_org.pem&quot;
chef_server_url          &quot;https://ChefServer/organizations/xxxxxxxxx&quot;
syntax_check_cache_path  &quot;#{ENV['HOME']}/.chef/syntaxcache&quot;
cookbook_path            [&quot;#{current_dir}/../cookbooks&quot;]
$ knife ssl fetch
WARNING: Certificates from ChefServer will be fetched and placed in your trusted_cert
directory (/home/dash/chef-repo/.chef/trusted_certs).

Knife has no means to verify these are the correct certificates. You should
verify the authenticity of these certificates after downloading.

Adding certificate for ChefServer in /home/xxxx/chef-repo/.chef/trusted_certs/ChefServer.crt
$ knife ssl check
Connecting to host ChefServer:443
Successfully verified certificates from `ChefServer'
</code></pre><p>Check how many clients has been added into the ChefServer, currently only one,</p><pre><code>$ knife client list
xxxxxx-validator
</code></pre><h3 id=added-nodes>Added Nodes</h3><p>In Client1, Install the</p><pre><code>$ sudo dpkg -i chef_12.3.0-1_amd64.deb 
</code></pre><p>Add the pem files to every nodes:</p><pre><code># knife bootstrap Client1 -x xxxxxx -P XXXXXXXXXXXXX --sudo
</code></pre><p>If above steps fail, you should manually specify the ssl verification.</p><pre><code># scp Server/xxx.pem /home/xxxxx
# cp /home/xxxx/xxx.pem /etc/chef/validation.pem
# sudo chef-client -l debug -S https://ChefServer/organizations/xxxxx -K /etc/chef/validation.pem
##### OR
#  sudo chef-client -l debug -S https://ChefServer/organizations/xxxxx  -K /home/xxxx/xxxxx.pem
</code></pre><p>Bootstrap again:</p><pre><code># knife bootstrap Client1  -N ChefClient1 -x xxxxx -P xxxxxx --sudo --use-sudo-password
</code></pre><p>After bootstrap success, list all of the client:</p><pre><code>root@ChefWorkstation:~/chef-repo# knife client list
ChefClient1                                                                                                                                
xxxx-validator 
</code></pre><h3 id=using-cookbook>Using Cookbook</h3><p>Create the Cookbook named <code>nginx</code>:</p><pre><code>root@ChefWorkstation:~# cd chef-repo/
root@ChefWorkstation:~/chef-repo# ls
chefignore  cookbooks  data_bags  environments  LICENSE  README.md  roles
root@ChefWorkstation:~/chef-repo# knife cookbook create nginx
oot@ChefWorkstation:~/chef-repo/cookbooks/nginx# ls
attributes  CHANGELOG.md  definitions  files  libraries  metadata.rb  providers  README.md  recipes  resources  templates
</code></pre><p>Edit the cookbook:</p><p>Enable the installation:</p><pre><code># vim recipes/default.rb
package 'nginx' do
  action :install
end
</code></pre><p>Enable check the status:</p><pre><code>service 'nginx' do
  action [ :enable, :start ]
end
</code></pre><p>Change the index.html file:</p><pre><code>cookbook_file &quot;/usr/share/nginx/html/index.html&quot; do
  source &quot;index.html&quot;
  mode &quot;0644&quot;
end
</code></pre><p>Prepare the default index.html file:</p><pre><code>$ cd ~/chef-repo/cookbooks/nginx/files/default
$ vim index.html
&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;Hello there&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h1&gt;This is a test&lt;/h1&gt;
    &lt;p&gt;Please work!&lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre><p>Since the nginx need apt-get to achive the latest status, add another package named apt:</p><pre><code>knife cookbook create apt
</code></pre><p>Edit the default rb file:</p><pre><code>vim ~/chef-repo/cookbooks/apt/recipes/default.rb
execute &quot;apt-get update&quot; do
  command &quot;apt-get update&quot;
end
</code></pre><p>Change the default rb file of the nginx:</p><pre><code>+++ include_recipe &quot;apt&quot;

package 'nginx' do
  action :install
end
</code></pre><p>Also add it to the metadata.rb file:</p><pre><code>$ vim ~/chef-repo/cookbooks/nginx/metadata.rb

long_description IO.read(File.join(File.dirname(__FILE__), 'README.md'))
version          '0.1.0'

+++  depends &quot;apt&quot;
</code></pre><p>Add Cookbook to your nodes:</p><pre><code>knife cookbook upload apt
knife cookbook upload nginx
</code></pre><p>Or</p><pre><code>knife cookbook upload -a
</code></pre><p>Edit the specified node:</p><pre><code>knife node edit name_of_node

{
  &quot;name&quot;: &quot;client1&quot;,
  &quot;chef_environment&quot;: &quot;_default&quot;,
  &quot;normal&quot;: {
    &quot;tags&quot;: [

    ]
  },
  &quot;run_list&quot;: [

+++ &quot;recipe[name_of_recipe1]&quot;, 
+++ &quot;recipe[name_of_recipe2]&quot; 

  ]
}
</code></pre><p>In every want-to-deploy nodes, run:</p><pre><code>$ sudo chef-client
</code></pre><h3 id=use--market>Use Market</h3><p>Download and use the knife</p><pre><code>$ knife cookbook site download learn_chef_apache2
$ tar xzvf learn_chef_apache2-0.2.1.tar.gz -C cookbooks/
$ knife cookbook  upload -a 
</code></pre><p>Besure to edit the node&rsquo;s recipes.</p><p>Two tips:</p><p>Remove the cookbook from the server&rsquo;s list:</p><pre><code># knife cookbook delete learn_chef_apache2 0.2.1
</code></pre><p>Directly remove the recipe from the node:</p><pre><code># knife node run_list remove ChefClient1 recipe[nginx]
# knife node run_list remove ChefClient1 recipe[eclipse]
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/05/25/tips-on-deleteing-neutron-subnet-and-router/>Tips on deleteing neutron subnet and router</a></h1><span class=post-date>May 25, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>Get the existing subnet:</p><pre><code>root@Controller:~# neutron subnet-list 
+--------------------------------------+-------------+----------------+--------------------------------------------------+
| id                                   | name        | cidr           | allocation_pools                                 |
+--------------------------------------+-------------+----------------+--------------------------------------------------+
| 98725e3a-7ee2-4e3f-83e3-eaca0236918f | demo-subnet | 192.168.1.0/24 | {&quot;start&quot;: &quot;192.168.1.2&quot;, &quot;end&quot;: &quot;192.168.1.254&quot;} |
+--------------------------------------+-------------+----------------+--------------------------------------------------+
</code></pre><p>Delete it via:</p><pre><code>root@Controller:~# neutron subnet-delete --name demo-subnet
Unable to complete operation on subnet 98725e3a-7ee2-4e3f-83e3-eaca0236918f. One or more ports have an IP allocation from this subnet. (HTTP 409) (Request-ID: req-7d729bcc-ec50-4de6-83d9-5d2b98332127)
</code></pre><p>Because we have the router, so we list the router via:</p><pre><code>root@Controller:~# neutron router-list
+--------------------------------------+-------------+-----------------------+
| id                                   | name        | external_gateway_info |
+--------------------------------------+-------------+-----------------------+
| a745487e-8e7c-4cc2-aff7-a8423d0a6614 | demo-router | null                  |
+--------------------------------------+-------------+-----------------------+
</code></pre><p>Get the ports of this router:</p><pre><code>root@Controller:~# neutron router-port-list a745487e-8e7c-4cc2-aff7-a8423d0a6614
+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                          |
+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+
| e56fe57e-e939-493b-8984-b5adfa64e2cc |      | fa:16:3e:b3:7b:e6 | {&quot;subnet_id&quot;: &quot;98725e3a-7ee2-4e3f-83e3-eaca0236918f&quot;, &quot;ip_address&quot;: &quot;192.168.1.1&quot;} |
+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+

</code></pre><p>So we remove the interface from this router via:</p><pre><code>root@Controller:~# neutron router-interface-delete demo-router 98725e3a-7ee2-4e3f-83e3-eaca0236918f
Removed interface from router demo-router.
root@Controller:~# neutron router-port-list a745487e-8e7c-4cc2-aff7-a8423d0a6614
</code></pre><p>Now we could remove the router and the subnet:</p><pre><code>root@Controller:~# neutron router-delete demo-router
Deleted router: demo-router
root@Controller:~# neutron subnet-delete demo-subnet
Deleted subnet: demo-subnet
</code></pre><p>From now on ,you could create another subnet and router.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/05/25/san-jie-dian-da-jian-openstack-juno-4/>三节点搭建OpenStack Juno(4)</a></h1><span class=post-date>May 25, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>Neutron和nova-network的区别在于，nova-network可以让你在每个instance上部署一种网络类型，适合基本的网络功能。而Neutron则使得你可以在一个instance上部署多种网络类型，并且以插件的方式支持多种虚拟化网络。</p><p>详细的介绍，以后慢慢加，理解吃透了再加上来，这里单单提操作步骤。</p><h3 id=准备>准备</h3><p>数据库准备如下:</p><pre><code>root@Controller:~# mysql -u root -p
Enter password: 
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 39
Server version: 5.5.43-MariaDB-1ubuntu0.14.04.2 (Ubuntu)

Copyright (c) 2000, 2015, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE neutron;
Query OK, 1 row affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'xxxxx';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'xxxxx';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; quit;
Bye

</code></pre><h3 id=配置服务>配置服务</h3><pre><code>root@Controller:~# source admin-openrc.sh
root@Controller:~# keystone user-create --name neutron --pass xxxxx
+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|  email   |                                  |
| enabled  |               True               |
|    id    | a6d790e8e86749bba1d27972de8eaae2 |
|   name   |             neutron              |
| username |             neutron              |
+----------+----------------------------------+
root@Controller:~# keystone user-role-add --user neutron --tenant service --role admin
root@Controller:~# keystone service-create --name neutron --type network --description &quot;OpenStack Networking&quot;
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |       OpenStack Networking       |
|   enabled   |               True               |
|      id     | 2f6de710ec414797a4a639c2310c8249 |
|     name    |             neutron              |
|     type    |             network              |
+-------------+----------------------------------+
root@Controller:~# keystone endpoint-create --service-id $(keystone service-list | awk '/ network / {print $2}') --publicurl http://Controller:9696 --adminurl http://Controller:9696 --internalurl http://Controller:9696 --region regionOne
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
|   adminurl  |      http://Controller:9696      |
|      id     | a23132fd0a824aa09f3b1ea72cbb97d2 |
| internalurl |      http://Controller:9696      |
|  publicurl  |      http://Controller:9696      |
|    region   |            regionOne             |
|  service_id | 2f6de710ec414797a4a639c2310c8249 |
+-------------+----------------------------------+

</code></pre><h3 id=安装和配置网络组件>安装和配置网络组件</h3><p>安装下列包:</p><pre><code># apt-get install neutron-server neutron-plugin-ml2 python-neutronclient
</code></pre><p>在Controller节点上开始配置:</p><pre><code>$ sudo vim /etc/neutron/neutron.conf
[database]
...
connection = mysql://neutron:NEUTRON_DBPASS@Controller/neutron
</code></pre><p>配置rabbitMQ认证方式:</p><pre><code>
[DEFAULT]
...
rpc_backend = rabbit
rabbit_host = Controller
rabbit_password = RABBIT_PASS
</code></pre><p>配置keystone认证:</p><pre><code>[DEFAULT]
...
auth_strategy = keystone

#### 删除已有的keystone authtoken配置方式
[keystone_authtoken]
...
auth_uri = http://Controller:5000/v2.0
identity_uri = http://Controller:35357
admin_tenant_name = service
admin_user = neutron
admin_password = NEUTRON_PASS
</code></pre><p>激活ML2插件(Modular Layer 2), router服务, 和overlapping IP地址:</p><pre><code>[DEFAULT]
...
core_plugin = ml2
service_plugins = router
allow_overlapping_ips = True
</code></pre><p>配置网络，以便在网络拓扑发生变化时告知Compute节点:</p><pre><code>[DEFAULT]
...
notify_nova_on_port_status_changes = True
notify_nova_on_port_data_changes = True
nova_url = http://Controller:8774/v2
nova_admin_auth_url = http://Controller:35357/v2.0
nova_region_name = regionOne
nova_admin_username = nova
nova_admin_tenant_id = SERVICE_TENANT_ID
nova_admin_password = NOVA_PASS
</code></pre><p><code>SERVICE_TENANT_ID</code> 通过以下命令来获得:</p><pre><code>$ source admin-openrc.sh
$ keystone tenant-get service
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |          Service Tenant          |
|   enabled   |               True               |
|      id     | 08a675be93a04cca8a74159a3eefa288 |
|     name    |             service              |
+-------------+----------------------------------+
</code></pre><p>可以自定义打开verbose选项:</p><pre><code>[DEFAULT]
...
verbose = True
</code></pre><p>配置Modular Layer2(ML2)插件:</p><p>在[ml2]部分，激活flat and generic routing encapsulation(GRE) network类型驱动, GRE Tenant网络和OVS机制驱动:</p><pre><code>$ sudo vim /etc/neutron/plugins/ml2/ml2_conf.ini
[ml2]
...
type_drivers = flat,gre
tenant_network_types = gre
mechanism_drivers = openvswitch
</code></pre><p><code>[ml2_type_gre]</code> 部分，配置tunnel identifier(id)范围:</p><pre><code>[ml2_type_gre]
...
tunnel_id_ranges = 1:1000
</code></pre><p>配置securitygroup部分,</p><pre><code>[securitygroup]
...
enable_security_group = True
enable_ipset = True
firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
</code></pre><p>配置计算服务使用网络:</p><pre><code>$ sudo vim /etc/nova/nova.conf
[DEFAULT]
...
network_api_class = nova.network.neutronv2.api.API
security_group_api = neutron
linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver
firewall_driver = nova.virt.firewall.NoopFirewallDriver
</code></pre><p>在<code>[neutron]</code>部分，配置访问参数:</p><pre><code>[neutron]
...
url = http://Controller:9696
auth_strategy = keystone
admin_auth_url = http://Controller:35357/v2.0
admin_tenant_name = service
admin_username = neutron
admin_password = NEUTRON_PASS
</code></pre><p>完成安装：</p><pre><code># su -s /bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade juno&quot; neutron
# service nova-api restart
# service nova-scheduler restart
# service nova-conductor restart
# service neutron-server restart

</code></pre><p>验证:</p><pre><code>root@Controller:~# source admin-openrc.sh
root@Controller:~# neutron ext-list
+-----------------------+-----------------------------------------------+
| alias                 | name                                          |
+-----------------------+-----------------------------------------------+
| security-group        | security-group                                |
| l3_agent_scheduler    | L3 Agent Scheduler                            |
| ext-gw-mode           | Neutron L3 Configurable external gateway mode |
| binding               | Port Binding                                  |
| provider              | Provider Network                              |
| agent                 | agent                                         |
| quotas                | Quota management support                      |
| dhcp_agent_scheduler  | DHCP Agent Scheduler                          |
| l3-ha                 | HA Router extension                           |
| multi-provider        | Multi Provider Network                        |
| external-net          | Neutron external network                      |
| router                | Neutron L3 Router                             |
| allowed-address-pairs | Allowed Address Pairs                         |
| extraroute            | Neutron Extra Route                           |
| extra_dhcp_opt        | Neutron Extra DHCP opts                       |
| dvr                   | Distributed Virtual Router                    |
+-----------------------+-----------------------------------------------+

</code></pre><h3 id=配置网络节点>配置网络节点</h3><p>安装以下包:</p><pre><code># apt-get install neutron-plugin-ml2 neutron-plugin-openvswitch-agent neutron-l3-agent neutron-dhcp-agent
</code></pre><p>配置， 首先，删除<code>/etc/neutron/neutron.conf</code>里所有的database连接，因为网络节点不需要任何数据库连接。</p><pre><code>$ sudo vim /etc/neutron/neutron.conf
[DEFAULT]
...
rpc_backend = rabbit
rabbit_host = controller
rabbit_password = RABBIT_PASS
</code></pre><p>更改keystone认证方式:</p><pre><code>[DEFAULT]
...
auth_strategy = keystone
[keystone_authtoken]
...
auth_uri = http://controller:5000/v2.0
identity_uri = http://controller:35357
admin_tenant_name = service
admin_user = neutron
admin_password = NEUTRON_PASS
</code></pre><p>有关<code>[ml2]</code>的配置:</p><pre><code>[DEFAULT]
...
core_plugin = ml2
service_plugins = router
allow_overlapping_ips = True
</code></pre><p>接着：</p><pre><code>$ sudo vim /etc/neutron/plugins/ml2/ml2_conf
[ml2]
...
type_drivers = flat,gre
tenant_network_types = gre
mechanism_drivers = openvswitch
[ml2_type_flat]
...
flat_networks = external
[ml2_type_gre]
...
tunnel_id_ranges = 1:1000
[securitygroup]
...
enable_security_group = True
enable_ipset = True
firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
[ovs]
...
local_ip = INSTANCE_TUNNELS_INTERFACE_IP_ADDRESS
enable_tunneling = True
bridge_mappings = external:br-ex
[agent]
...
tunnel_types = gre
</code></pre><p>配置Layer-3客户端:</p><pre><code>$ sudo vim /etc/neutron/l3_agent.ini
[DEFAULT]
...
interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver
use_namespaces = True
external_network_bridge = br-ex
router_delete_namespaces = True

</code></pre><p>配置DHCP客户端：</p><pre><code>$ sudo vim /etc/neutron/dhcp_agent.init
[DEFAULT]
...
interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
use_namespaces = True
dhcp_delete_namespaces = True

[DEFAULT]
...
dnsmasq_config_file = /etc/neutron/dnsmasq-neutron.conf
</code></pre><p>配置dnsmasq配置文件:</p><pre><code>$ sudo vim /etc/neutron/dnsmasq_neutron.conf
dhcp-option-force=26,1454
$ sudo pkill dnsmasq
</code></pre><p>配置metadata客户端:</p><pre><code>$ sudo vim /etc/neutron/metadata_agent.ini
[DEFAULT]
...
auth_url = http://controller:5000/v2.0
auth_region = regionOne
admin_tenant_name = service
admin_user = neutron
admin_password = NEUTRON_PASS

[DEFAULT]
...
nova_metadata_ip = controller

[DEFAULT]
...
metadata_proxy_shared_secret = METADATA_SECRET


</code></pre><p>对应的，在控制节点上，配置:</p><pre><code>$ sudo vim /etc/nova/nova.conf
[neutron]
...
service_metadata_proxy = True
metadata_proxy_shared_secret = METADATA_SECRET
# service nova-api restart

</code></pre><p>配置Open vSwitch(OVS)服务:</p><pre><code># service openvswitch-switch restart
# ovs-vsctl add-br br-ex
# ovs-vsctl add-port br-ex INTERFACE_NAME
# service neutron-plugin-openvswitch-agent restart
# service neutron-l3-agent restart
# service neutron-dhcp-agent restart
# service neutron-metadata-agent restart
</code></pre><p>验证:</p><pre><code>$ source admin-openrc.sh
$ neutron agent-list

</code></pre><h3 id=计算节点配置>计算节点配置</h3><p>配置如下;</p><pre><code>$ sudo vim /etc/sysctl.conf
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.rp_filter=0
# sysctl -p
</code></pre><p>安装网络组件：</p><pre><code># apt-get install neutron-plugin-ml2 neutron-plugin-openvswitch-agent
</code></pre><p>配置网络组件：</p><pre><code>$ sudo vim /etc/neutron/neutron.conf
[DEFAULT]
...
rpc_backend = rabbit
rabbit_host = controller
rabbit_password = RABBIT_PASS
</code></pre><p>keystone组件:</p><pre><code>[DEFAULT]
...
auth_strategy = keystone
[keystone_authtoken]
...
auth_uri = http://controller:5000/v2.0
identity_uri = http://controller:35357
admin_tenant_name = service
admin_user = neutron
admin_password = NEUTRON_PASS
</code></pre><p><code>[ml2]</code>插件:</p><pre><code>$ sudo vim /etc/neutron/neutron.conf
[DEFAULT]
...
core_plugin = ml2
service_plugins = router
allow_overlapping_ips = True
</code></pre><p>继续配置ml2插件:</p><pre><code>$ sudo vim /etc/neutron/plugins/ml2/ml2_conf.ini
[ml2]
...
type_drivers = flat,gre
tenant_network_types = gre
mechanism_drivers = openvswitch

[ml2_type_gre]
...
tunnel_id_ranges = 1:1000

[securitygroup]
...
enable_security_group = True
enable_ipset = True
firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver

[ovs]
...
local_ip = INSTANCE_TUNNELS_INTERFACE_IP_ADDRESS
enable_tunneling = True

[agent]
...
tunnel_types = gre
</code></pre><p>配置OVS服务：</p><pre><code># service openvswitch-switch restart

</code></pre><p>配置计算节点使用网络:</p><pre><code>$ sudo vim /etc/nova/nova.conf
[DEFAULT]
...
network_api_class = nova.network.neutronv2.api.API
security_group_api = neutron
linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver
firewall_driver = nova.virt.firewall.NoopFirewallDriver

[neutron]
...
url = http://controller:9696
auth_strategy = keystone
admin_auth_url = http://controller:35357/v2.0
admin_tenant_name = service
admin_username = neutron
admin_password = NEUTRON_PASS
</code></pre><p>完成安装:</p><pre><code># service nova-compute restart
# service neutron-plugin-openvswitch-agent restart
</code></pre><p>验证， 在Controller节点上:</p><pre><code>$ source admin-openrc.sh
$ neutron agent-list
</code></pre><h3 id=创建初始化网络>创建初始化网络</h3><p>步骤如下:<br>创建外网:</p><pre><code># source admin-openrc.sh
# neutron net-create ext-net --router:external True \
--provider:physical_network external --provider:network_type flat
# neutron subnet-create ext-net --name ext-subnet \
--allocation-pool start=10.77.77.200,end=10.77.77.220 \
--disable-dhcp --gateway 10.77.77.1 10.77.77.0/24
</code></pre><p>租户网络:</p><pre><code>$ source demo-openrc.sh
$ neutron net-create demo-net
$ neutron subnet-create demo-net --name demo-subnet \
--gateway 10.10.10.1 10.10.10.0/24
$ neutron router-create demo-router
$ neutron router-interface-add demo-router demo-subnet
$ neutron router-gateway-set demo-router ext-net
</code></pre><p>验证, 因为我们用了10.77.77.200~220作为外网的floating IP, 所以路由器外网的IP应该落在10.77.77.200上，在计算节点上直接ping, 看结果:</p><pre><code>dash@PowerfulDash:~$ ping 10.77.77.200
PING 10.77.77.200 (10.77.77.200) 56(84) bytes of data.
64 bytes from 10.77.77.200: icmp_seq=1 ttl=64 time=0.323 ms
64 bytes from 10.77.77.200: icmp_seq=2 ttl=64 time=0.177 ms
64 bytes from 10.77.77.200: icmp_seq=3 ttl=64 time=0.141 ms
</code></pre><p>其实上面的结果是在Host机器上Ping的，因host机器已经有了10.77.77.1地址，理论上，如果能Ping通10.77.77.200，证明Router工作正常。</p><h3 id=horizon>Horizon</h3><p>在控制节点上安装以下包 ：</p><pre><code># apt-get install openstack-dashboard apache2 libapache2-mod-wsgi memcached python-memcache
</code></pre><p>配置:</p><pre><code># vim /etc/openstack-dashboard/local_settings.py
OPENSTACK_HOST = &quot;controller&quot;
ALLOWED_HOSTS = ['*']
CACHES = {
'default': {
'BACKEND': 'django.core.cache.backends.memcached.
MemcachedCache',
'LOCATION': '127.0.0.1:11211',
}
}
TIME_ZONE = &quot;TIME_ZONE&quot;

</code></pre><p>重启服务:</p><pre><code># service apache2 restart
# service memcached restart
</code></pre><p>最后访问:<br><a href=http://Controller/horizon>http://Controller/horizon</a> 来看到结果.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/05/25/san-jie-dian-da-jian-openstack-juno-3/>三节点搭建OpenStack Juno(3)</a></h1><span class=post-date>May 25, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=nova>Nova</h3><h4 id=nova数据库>Nova数据库</h4><p>创建nova数据库:</p><pre><code># mysql -u root -p
	CREATE DATABASE nova;
	GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' \
	IDENTIFIED BY 'NOVA_DBPASS';
	GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' \
	IDENTIFIED BY 'NOVA_DBPASS';
	quit;
</code></pre><p>创建nova用户:</p><pre><code># source /home/dash/admin-openrc.sh
root@Controller:~# keystone user-create --name nova --pass xxxxxx
+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|  email   |                                  |
| enabled  |               True               |
|    id    | 4a3768e3f4754cd0b9d47c6fadb22c7e |
|   name   |               nova               |
| username |               nova               |
+----------+----------------------------------+
</code></pre><p>为admin角色添加nova用户:</p><pre><code># keystone user-role-add --user nova --tenant service --role admin
</code></pre><p>添加nova服务条目:</p><pre><code># keystone service-create --name nova --type compute --description &quot;OpenStack Compute&quot;
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |        OpenStack Compute         |
|   enabled   |               True               |
|      id     | 1587a46ee1e94402821398444175981f |
|     name    |               nova               |
|     type    |             compute              |

</code></pre><p>创建Compute服务的API endpoints:</p><pre><code># keystone endpoint-create --service-id $(keystone service-list | awk '/ compute / {print $2}') --publicurl http://Controller:8774/v2/%\(tenant_id\)s --internalurl http://Controller:8774/v2/%\(tenant_id\)s --adminurl http://Controller:8774/v2/%\(tenant_id\)s --region regionOne
+-------------+-----------------------------------------+
|   Property  |                  Value                  |
+-------------+-----------------------------------------+
|   adminurl  | http://Controller:8774/v2/%(tenant_id)s |
|      id     |     bd439dc236c04956a11b353a7b74331c    |
| internalurl | http://Controller:8774/v2/%(tenant_id)s |
|  publicurl  | http://Controller:8774/v2/%(tenant_id)s |
|    region   |                regionOne                |
|  service_id |     1587a46ee1e94402821398444175981f    |
+-------------+-----------------------------------------+
</code></pre><h4 id=nova安装及配置>Nova安装及配置</h4><p>安装以下包:</p><pre><code># apt-get install nova-api nova-cert nova-conductor nova-consoleauth nova-novncproxy nova-scheduler python-novaclient
</code></pre><p>更改数据库链接:</p><pre><code>$ sudo vim /etc/nova/nova.conf
[database]
...
connection = mysql://nova:NOVA_DBPASS@controller/nova
</code></pre><p>配置RabbitMQ访问:</p><pre><code>[DEFAULT]
...
rpc_backend = rabbit
rabbit_host = controller
rabbit_password = RABBIT_PASS
</code></pre><p>配置鉴权服务:</p><pre><code>[DEFAULT]
...
auth_strategy = keystone
[keystone_authtoken]
...
auth_uri = http://controller:5000/v2.0
identity_uri = http://controller:35357
admin_tenant_name = service
admin_user = nova
admin_password = NOVA_PASS
</code></pre><p>更改<code>my_ip</code>:</p><pre><code>[DEFAULT]
...
my_ip = 10.55.55.2
</code></pre><p>更改VNC侦听：</p><pre><code>[DEFAULT]
...
vncserver_listen = 10.55.55.2
vncserver_proxyclient_address = 10.55.55.2
</code></pre><p>配置glance服务所在:</p><pre><code>[glance]
...
host = controller
</code></pre><p>现在开始配置数据库:</p><pre><code># su -s /bin/sh -c &quot;nova-manage db sync&quot; nova
</code></pre><p>重启服务以完成安装:</p><pre><code># service nova-api restart
# service nova-cert restart
# service nova-consoleauth restart
# service nova-scheduler restart 
# service nova-conductor start
# service nova-conductor restart
# service nova-novncproxy
# service nova-novncproxy restart
</code></pre><p>扫尾，删除不用的sqlite数据库:</p><pre><code># rm -f /var/lib/nova/nova.sqlite
</code></pre><h3 id=安装和配置计算节点>安装和配置计算节点</h3><p>在计算节点上，安装以下包:</p><pre><code># apt-get install nova-compute sysfsutils
</code></pre><p>配置具体过程如下:<br>配置RabbitMQ:</p><pre><code>$ sudo vim /etc/nova/nova.conf
[DEFAULT]
...
rpc_backend = rabbit
rabbit_host = controller
rabbit_password = RABBIT_PASS
</code></pre><p>配置鉴权服务:</p><pre><code>[DEFAULT]
...
auth_strategy = keystone
[keystone_authtoken]
...
auth_uri = http://Controller:5000/v2.0
identity_uri = http://Controller:35357
admin_tenant_name = service
admin_user = nova
admin_password = NOVA_PASS
</code></pre><p>配置<code>my_ip</code>:</p><pre><code>[DEFAULT]
...
my_ip = 10.55.55.4
</code></pre><p>配置允许远程终端访问:</p><pre><code>[DEFAULT]
...
vnc_enabled = True
vncserver_listen = 0.0.0.0
vncserver_proxyclient_address = 10.55.55.4
novncproxy_base_url = http://Controller:6080/vnc_auto.html

</code></pre><p>配置glance服务:</p><pre><code>[glance]
...
host = Controller
</code></pre><p>完成安装， 首先判断你的CPU是否支持硬件加速:</p><pre><code>$ egrep -c '(vmx|svm)' /proc/cpuinfo
</code></pre><p>如果返回的值小于1, 则更改<code>/etc/nova/nova-compute.conf</code>配置中的[libvirt]选项，选用qemu而不是kvm:</p><pre><code>$ sudo vim /etc/nova/nova-compute.conf
[libvirt]
...
virt_type = qemu
</code></pre><p>重启nova-compute服务:</p><pre><code># service nova-compute restart
</code></pre><p>扫尾，删除不用的nova.sqlite文件:</p><pre><code># rm -f /var/lib/nova/nova.sqlite
</code></pre><h3 id=验证>验证</h3><p>具体步骤如下:</p><pre><code>root@Controller:~# source ~/admin-openrc.sh
root@Controller:~# nova service-list
+----+------------------+------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host       | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-cert        | Controller | internal | enabled | up    | 2015-05-25T12:00:20.000000 | -               |
| 2  | nova-consoleauth | Controller | internal | enabled | up    | 2015-05-25T12:00:28.000000 | -               |
| 3  | nova-scheduler   | Controller | internal | enabled | up    | 2015-05-25T12:00:23.000000 | -               |
| 4  | nova-conductor   | Controller | internal | enabled | up    | 2015-05-25T12:00:25.000000 | -               |
| 5  | nova-compute     | Compute    | nova     | enabled | up    | 2015-05-25T12:00:20.000000 | -               |
+----+------------------+------------+----------+---------+-------+----------------------------+-----------------+
root@Controller:~# nova image-list
+--------------------------------------+---------------------+--------+--------+
| ID                                   | Name                | Status | Server |
+--------------------------------------+---------------------+--------+--------+
| 3d45ea58-731c-4eb5-bf30-db1b4bfe4f57 | cirros-0.3.3-x86_64 | ACTIVE |        |
+--------------------------------------+---------------------+--------+--------+

</code></pre><p>注意我们看到了compute节点已经被加入进来。这样我们完成了添加计算服务，接下来我们将开始添加网络组件，这可能是最难的一部分。</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/05/25/lxcize-the-kvm-machine/>LXCize the KVM machine</a></h1><span class=post-date>May 25, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>In order to make exsiting kvm based machine to be lxc container, following is the steps.<br>Refers to:<br><a href=https://www.stgraber.org/2012/03/04/booting-an-ubuntu-12-04-virtual-machine-in-an-lxc-container/>https://www.stgraber.org/2012/03/04/booting-an-ubuntu-12-04-virtual-machine-in-an-lxc-container/</a></p><h3 id=convert-disk-formats>Convert Disk Formats</h3><p>First we want to convert the qcow2 format image to raw format, by following command:</p><pre><code>$ qemu-img convert u12-debug-ui.qcow2 Contrail.raw
</code></pre><p>This will take a very long time, because qcow2 file will expand to a whole images, like mine, the Contrail.raw in fact expands to 100G size.</p><h3 id=lxc-the-kvm>LXC the KVM</h3><p>Since we have the raw image file, we could create a new configuration file for starting the machine:</p><pre><code> myvm.conf
lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.name = eth0
lxc.network.ipv4 = xxx.xxx.10.230/24
lxc.network.ipv4.gateway = xxx.xxx.0.176
#lxc.network.link = lxcbr0
lxc.utsname = myvminlxc

lxc.tty = 4
lxc.pts = 1024
lxc.rootfs = /dev/mapper/loop0p1
lxc.arch = amd64
lxc.cap.drop = sys_module mac_admin

lxc.cgroup.devices.deny = a
# Allow any mknod (but not using the node)
lxc.cgroup.devices.allow = c *:* m
lxc.cgroup.devices.allow = b *:* m
# /dev/null and zero
lxc.cgroup.devices.allow = c 1:3 rwm
lxc.cgroup.devices.allow = c 1:5 rwm
# consoles
lxc.cgroup.devices.allow = c 5:1 rwm
lxc.cgroup.devices.allow = c 5:0 rwm
#lxc.cgroup.devices.allow = c 4:0 rwm
#lxc.cgroup.devices.allow = c 4:1 rwm
# /dev/{,u}random
lxc.cgroup.devices.allow = c 1:9 rwm
lxc.cgroup.devices.allow = c 1:8 rwm
lxc.cgroup.devices.allow = c 136:* rwm
lxc.cgroup.devices.allow = c 5:2 rwm
# rtc
lxc.cgroup.devices.allow = c 254:0 rwm
#fuse
lxc.cgroup.devices.allow = c 10:229 rwm
#tun
lxc.cgroup.devices.allow = c 10:200 rwm
#full
lxc.cgroup.devices.allow = c 1:7 rwm
#hpet
lxc.cgroup.devices.allow = c 10:228 rwm
#kvm
lxc.cgroup.devices.allow = c 10:232 rwm
</code></pre><p>Now setup the image file via, this will use the <code>/dev/mapper/loop0p1</code> as the root partition:</p><pre><code>$ sudo kpartx -a Contrail.img
</code></pre><p>Now start the machine via:</p><pre><code>$ sudo lxc-start -n myvminlxc -f ./myvm.conf
</code></pre><p>Your machine will boot into the lxc, and its behavior is the same as the kvm based machine.</p><h3 id=troubleshooting>TroubleShooting</h3><p>The root could not login into the lxc, because of the selinux enabled in the Ubuntu Host, simply disable it in <code>/etc/selinux/config</code> by:</p><pre><code>$ sudo vim /etc/selinux/config
#SELINUX=permissive
SELINUX=disabled

</code></pre><p>Added it into the startup file in <code>/etc/rc.local</code>, it&rsquo;s ugly, and it will cause the first tty died. But, first use it:</p><pre><code>kpartx -a /home/xxxxx/iso/Contrail.raw
lxc-start -n myvminlxc -f /home/xxxxx/iso/myvm.conf

</code></pre></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/131/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/131/>131</a></li><li class="page-item active"><a class=page-link href=/page/132/>132</a></li><li class=page-item><a class=page-link href=/page/133/>133</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/226/>226</a></li><li class=page-item><a href=/page/133/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/226/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>