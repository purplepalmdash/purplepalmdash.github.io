<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/12/17/netscaler-vpxchu-shi-hua-pei-zhi/>NetScaler VPX初始化配置</a></h1><span class=post-date>Dec 17, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=初始化配置>初始化配置</h3><p>启动虚拟机以后，通过nsroot/nsroot登录入VPX.</p><p>清除所有配置:</p><p><img src=/images/2015_12_17_14_20_54_580x93.jpg alt=/images/2015_12_17_14_20_54_580x93.jpg></p><p>如下，做IP配置:</p><p><img src=/images/2015_12_17_14_23_29_728x153.jpg alt=/images/2015_12_17_14_23_29_728x153.jpg></p><p>初始化配置完毕以后，即可在web后台进行配置。</p><h3 id=license>License</h3><p>申请license的时候注意，选择的MAC地址不能有任何的<code>:</code>符号， 例如52:54:00:这种就不能通过成
功。 在Netscaler上可以通过以下命令查看host id:</p><pre><code>root@ns# lmutil lmhostid 
lmutil - Copyright (c) 1989-2013 Flexera Software LLC. All Rights Reserved. 
The FlexNet host ID of this machine is &quot;xxxxxxx&quot;
</code></pre><p>查看激活后的license情形:</p><pre><code>&gt; sh license
        License status:
                           Web Logging: YES
                      Surge Protection: YES
                        Load Balancing: YES
                     Content Switching: YES
....
</code></pre><p>参考:<br><a href=http://sam.yeung.blog.163.com/blog/static/222663482013811102013782/>http://sam.yeung.blog.163.com/blog/static/222663482013811102013782/</a></p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-4/>用Graphite呈现广州空气质量(4)</a></h1><span class=post-date>Dec 16, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=tessera>Tessera</h3><p>直接导入Graphite中定义好的dashboard即可，值得注意的是，如何创建模板，或者说，如何创建一
个template用于渲染我们导入的各个数据？</p><p>导入的时候出现了如下的问题:</p><p><img src=/images/2015_12_16_18_34_44_701x483.jpg alt=/images/2015_12_16_18_34_44_701x483.jpg></p><p>可见tessera中对数据的定制化是必须的。</p><h3 id=grafana>Grafana</h3><p>安装及配置为自动启动:</p><pre><code>$ wget https://grafanarel.s3.amazonaws.com/builds/grafana_2.6.0_amd64.deb
$ sudo dpkg -i grafana_2.6.0_amd64.deb
$ sudo service grafana-server start
$ sudo update-rc.d grafana-server defaults 95 10
</code></pre><p>默认用户名/密码为 admin/admin.</p><p>现在添加graphite数据源，例如：</p><p><img src=/images/2015_12_16_19_20_33_703x484.jpg alt=/images/2015_12_16_19_20_33_703x484.jpg></p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-3/>用Graphite呈现广州空气质量(3)</a></h1><span class=post-date>Dec 16, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=当前节点无数据>当前节点无数据</h3><p>我们的脚本加入crontab运行后，最开始是可以得到数据的，后面两小时它挂了，查原因，有以下的
报错信息:</p><pre><code># /home/adminubuntu/GuangzhouPM25.py 
Traceback (most recent call last):
  File &quot;/home/adminubuntu/GuangzhouPM25.py&quot;, line 112, in &lt;module&gt;
    airdata = get_air_data(positionsets)
  File &quot;/home/adminubuntu/GuangzhouPM25.py&quot;, line 80, in get_air_data
    PM25 = int(pattern.match(soup.find('td',{'id': 'pmtow'}).contents[0]).group())
ValueError: invalid literal for int() with base 10: ''
</code></pre><p>此时selenium控制的浏览器停在以下图例:</p><p><img src=/images/2015_12_16_14_21_03_497x301.jpg alt=/images/2015_12_16_14_21_03_497x301.jpg></p><p>可以看到，如果当前节点的数据为<code>--</code>， 则我们的python脚本运行会出现问题。因而我们在代码中
要加入少量修改。</p><h3 id=错误处理>错误处理</h3><p>以下的代码更改添加了错误处理，如果该监测点的数值为空，则不提交任何数据:</p><pre><code>@@ -66,9 +66,9 @@ def get_air_data(positionsets):
   hourdata = {}
   # Calling selenium, need linux X
   browser = Firefox()
-  # Added 10 seconds for waiting page for loading.
-  time.delay(10)
   browser.get(URL)
+  # Added 10 seconds for waiting page for loading.
+  time.sleep(10)
   # Click button one-by-one
   for position in positionsets:
     # After clicking, should re-get the page_source.
@@ -78,33 +78,37 @@ def get_air_data(positionsets):
     soup = BeautifulSoup(page_source, 'html.parser')
     # pm2.5 value would be something like xx 微克/立方米, so we need an regex for
     # matching, example: print int(pattern.match(input).group())
-    PM25 = int(pattern.match(soup.find('td',{'id': 'pmtow'}).contents[0]).group())
-    PM25_iaqi = int(pattern.match(soup.find('td',{'id': 'pmtow_iaqi'}).contents[0]).group())
-    PM10 = int(pattern.match(soup.find('td',{'id': 'pmten'}).contents[0]).group())
-    PM10_iaqi = int(pattern.match(soup.find('td',{'id': 'pmten_iaqi'}).contents[0]).group())
-    SO2 = int(pattern.match(soup.find('td',{'id': 'sotwo'}).contents[0]).group())
-    SO2_iaqi = int(pattern.match(soup.find('td',{'id': 'sotwo_iaqi'}).contents[0]).group())
-    NO2 = int(pattern.match(soup.find('td',{'id': 'notwo'}).contents[0]).group())
-    NO2_iaqi = int(pattern.match(soup.find('td',{'id': 'notwo_iaqi'}).contents[0]).group())
-    # Special notice the CO would be float value
-    CO = float(floatpattern.match(soup.find('td',{'id': 'co'}).contents[0]).group())
-    CO_iaqi = int(pattern.match(soup.find('td',{'id': 'co_iaqi'}).contents[0]).group())
-    O3 = int(pattern.match(soup.find('td',{'id': 'othree'}).contents[0]).group())
-    O3_iaqi = int(pattern.match(soup.find('td',{'id': 'othree_iaqi'}).contents[0]).group())
-    hourdata_key = pinyin.get(position)
-    hourdata[hourdata_key] = []
-    hourdata[hourdata_key].append(PM25)
-    hourdata[hourdata_key].append(PM25_iaqi)
-    hourdata[hourdata_key].append(PM10)
-    hourdata[hourdata_key].append(PM10_iaqi)
-    hourdata[hourdata_key].append(SO2)
-    hourdata[hourdata_key].append(SO2_iaqi)
-    hourdata[hourdata_key].append(NO2)
-    hourdata[hourdata_key].append(NO2_iaqi)
-    hourdata[hourdata_key].append(CO)
-    hourdata[hourdata_key].append(CO_iaqi)
-    hourdata[hourdata_key].append(O3)
-    hourdata[hourdata_key].append(O3_iaqi)
+    try:
+      PM25 = int(pattern.match(soup.find('td',{'id': 'pmtow'}).contents[0]).group())
+      PM25_iaqi = int(pattern.match(soup.find('td',{'id': 'pmtow_iaqi'}).contents[0]).group())
+      PM10 = int(pattern.match(soup.find('td',{'id': 'pmten'}).contents[0]).group())
+      PM10_iaqi = int(pattern.match(soup.find('td',{'id': 'pmten_iaqi'}).contents[0]).group())
+      SO2 = int(pattern.match(soup.find('td',{'id': 'sotwo'}).contents[0]).group())
+      SO2_iaqi = int(pattern.match(soup.find('td',{'id': 'sotwo_iaqi'}).contents[0]).group())
+      NO2 = int(pattern.match(soup.find('td',{'id': 'notwo'}).contents[0]).group())
+      NO2_iaqi = int(pattern.match(soup.find('td',{'id': 'notwo_iaqi'}).contents[0]).group())
+      # Special notice the CO would be float value
+      CO = float(floatpattern.match(soup.find('td',{'id': 'co'}).contents[0]).group())
+      CO_iaqi = int(pattern.match(soup.find('td',{'id': 'co_iaqi'}).contents[0]).group())
+      O3 = int(pattern.match(soup.find('td',{'id': 'othree'}).contents[0]).group())
+      O3_iaqi = int(pattern.match(soup.find('td',{'id': 'othree_iaqi'}).contents[0]).group())
+      hourdata_key = pinyin.get(position)
+      hourdata[hourdata_key] = []
+      hourdata[hourdata_key].append(PM25)
+      hourdata[hourdata_key].append(PM25_iaqi)
+      hourdata[hourdata_key].append(PM10)
+      hourdata[hourdata_key].append(PM10_iaqi)
+      hourdata[hourdata_key].append(SO2)
+      hourdata[hourdata_key].append(SO2_iaqi)
+      hourdata[hourdata_key].append(NO2)
+      hourdata[hourdata_key].append(NO2_iaqi)
+      hourdata[hourdata_key].append(CO)
+      hourdata[hourdata_key].append(CO_iaqi)
+      hourdata[hourdata_key].append(O3)
+      hourdata[hourdata_key].append(O3_iaqi)
+    except ValueError, Argument:
+      # won't add the data, simply ignore this position
+      print &quot;The argument does not contain numbers\n&quot;, Argument
   # After clicking all of the button, quit the firefox and return the dictionary
   browser.close()
   return hourdata
</code></pre><p>到现在为止，数据可以顺利的写入到Graphite中。</p><h3 id=graphite-dashboard>Graphite Dashboard</h3><p>组建Graphite Dashboard可以通过图形界面来进行，举例如下:</p><p><img src=/images/2015_12_16_15_22_04_579x386.jpg alt=/images/2015_12_16_15_22_04_579x386.jpg></p><p>具体的添加过程就不说了，值得注意的是，设置几个属性，时间范围为过去24小时，
双击某图片后，<code>Render Options</code>里的<code>Line Mode</code>选择<code>Connected Line</code>，
这样可以构建出连接线，比较适合我们所需要展示的数据类型。Y-Axis，即Y轴的起点(Minimal)设置为0.</p><p>点击DashBoard-> Edit Dashboard, 可以看到以下定义:</p><p><img src=/images/2015_12_16_15_25_54_789x499.jpg alt=/images/2015_12_16_15_25_54_789x499.jpg></p><p>这个定义文件可以修改，我们将使用这个定义文件来批量制作其他十多个监测点的Dashboard.</p><h3 id=创建更多的dashboard>创建更多的Dashboard</h3><p>参考:<br><a href=http://graphite.readthedocs.org/en/latest/dashboard.html#editing-importing-and-exporting-via-json>http://graphite.readthedocs.org/en/latest/dashboard.html#editing-importing-and-exporting-via-json</a></p><p>将上述的dashboard定义文件存储在某个文本文件中，
用下列命令批量生成新的dashboard定义文件:</p><pre><code>$ cat dashboard.txt | sed 's/haizhuhu/aotizhongxin/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/aotizhongxin/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/baiyunshan/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/dafushan/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/gongyuanqian/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/haizhubaogang/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/haizhuchisha/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/haizhuhu/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/haizhushayuan/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/huangpudashadi/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/huangpuwenchong/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/huangshalubianzhan/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/liwanfangcun/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/liwanxicun/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/luhu/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/luogangxiqu/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/tianhelongdong/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/tiyuxi/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/yayuncheng/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/yangjilubianzhan/g'|myclip
</code></pre><p><code>myclip</code>是一个自定义的命令，可以将管道输出直接到系统剪贴板，
而后将内容新添加到dashboard定义文件中，点击update后，另存为新的dashboard即可.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-2/>用Graphite呈现广州空气质量(2)</a></h1><span class=post-date>Dec 16, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=更改后的脚本>更改后的脚本</h3><p>以下脚本可以用于取回网页上的数据，并将其写入到Graphite远程服务器。</p><pre><code>#!/usr/bin/env python
#-*-coding:utf-8 -*-

##################################################################################
# For fetching back the Air Quality Data and write it into Graphite on local server
# Graphite Data Definition, this is the general definition among every city
# air.city.citypoint.so2
# air.city.citypoint.no2
# air.city.citypoint.pm10
# air.city.citypoint.co
# air.city.citypoint.o38h
# air.city.citypoint.pm25
# air.city.citypoint.aqi
# air.city.citypoint.firstp
# air.city.citypoint.overp
# When running this script in crontab, be sure to give it a display
# Example, execute this script every hour at xx:05
# 5 */1 * * * export DISPLAY=:0;/home/adminubuntu/GuangzhouPM25.py
##################################################################################

# BeautifulSoup
from bs4 import BeautifulSoup

# Selenium
from contextlib import closing
from selenium.webdriver import Firefox
from selenium.webdriver.support.ui import WebDriverWait

# For writing into Graphite
import platform
import socket
import time

# Regex
import re

# pinyin
import pinyin

# Parameters comes here 
CARBON_SERVER = '0.0.0.0'
CARBON_PORT = 2003
DELAY = 5  # secs
URL = 'http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html'
CITY = 'guangzhou'

# All Points In Guangzhou City
positionsets = [&quot;天河龙洞&quot;, &quot;白云山&quot;, &quot;麓湖&quot;, &quot;公园前&quot;, &quot;荔湾西村&quot;, &quot;黄沙路边站&quot;, &quot;杨箕
路边站&quot;, &quot;荔湾芳村&quot;, &quot;海珠宝岗&quot;, &quot;海珠沙园&quot;, &quot;海珠湖&quot;, &quot;大夫山&quot;, &quot;奥体中心&quot;, &quot;萝岗西区
&quot;, &quot;黄埔文冲&quot;, &quot;黄埔大沙地&quot;, &quot;亚运城&quot;, &quot;体育西&quot;, &quot;海珠赤沙&quot;]

# regex for matching the digits.
pattern = re.compile(r'\d*')
floatpattern=re.compile(r'[\d|\.]*')

# Sending message to graphite server. 
def send_msg(message):
  print 'sending message:\n%s' % message
  sock = socket.socket()
  sock.connect((CARBON_SERVER, CARBON_PORT))
  sock.sendall(message)
  sock.close()

# Fetching data, runs each hour. In one-time access should fetch all of the data. 
def get_air_data(positionsets):
  # Dictionary hourdata is for holding data, DataStructure like: 
  # {'baiyunshan': [44, 5], 'haizhubaogang': [55, 6]}
  hourdata = {}
  # Calling selenium, need linux X
  browser = Firefox()
  browser.get(URL)
  # Click button one-by-one
  for position in positionsets:
    # After clicking, should re-get the page_source.
    browser.find_element_by_id(position).click()
    page_source = browser.page_source
    # Cooking Soup
    soup = BeautifulSoup(page_source, 'html.parser')
    # pm2.5 value would be something like xx 微克/立方米, so we need an regex for
    # matching, example: print int(pattern.match(input).group())
    PM25 = int(pattern.match(soup.find('td',{'id': 'pmtow'}).contents[0]).group())
    PM25_iaqi = int(pattern.match(soup.find('td',{'id':
'pmtow_iaqi'}).contents[0]).group())
    PM10 = int(pattern.match(soup.find('td',{'id': 'pmten'}).contents[0]).group())
    PM10_iaqi = int(pattern.match(soup.find('td',{'id':
'pmten_iaqi'}).contents[0]).group())
    SO2 = int(pattern.match(soup.find('td',{'id': 'sotwo'}).contents[0]).group())
    SO2_iaqi = int(pattern.match(soup.find('td',{'id':
'sotwo_iaqi'}).contents[0]).group())
    NO2 = int(pattern.match(soup.find('td',{'id': 'notwo'}).contents[0]).group())
    NO2_iaqi = int(pattern.match(soup.find('td',{'id':
'notwo_iaqi'}).contents[0]).group())
    # Special notice the CO would be float value
    CO = float(floatpattern.match(soup.find('td',{'id': 'co'}).contents[0]).group())
    CO_iaqi = int(pattern.match(soup.find('td',{'id': 'co_iaqi'}).contents[0]).group())
    O3 = int(pattern.match(soup.find('td',{'id': 'othree'}).contents[0]).group())
    O3_iaqi = int(pattern.match(soup.find('td',{'id':
'othree_iaqi'}).contents[0]).group())
    hourdata_key = pinyin.get(position)
    hourdata[hourdata_key] = []
    hourdata[hourdata_key].append(PM25)
    hourdata[hourdata_key].append(PM25_iaqi)
    hourdata[hourdata_key].append(PM10)
    hourdata[hourdata_key].append(PM10_iaqi)
    hourdata[hourdata_key].append(SO2)
    hourdata[hourdata_key].append(SO2_iaqi)
    hourdata[hourdata_key].append(NO2)
    hourdata[hourdata_key].append(NO2_iaqi)
    hourdata[hourdata_key].append(CO)
    hourdata[hourdata_key].append(CO_iaqi)
    hourdata[hourdata_key].append(O3)
    hourdata[hourdata_key].append(O3_iaqi)
  # After clicking all of the button, quit the firefox and return the dictionary
  browser.close()
  return hourdata

if __name__ == '__main__':
  airdata = get_air_data(positionsets)
  timestamp = int(time.time())
  for i in airdata.keys():
    # each key should contains the corresponding hourdata
    lines = [
      'air.guangzhou.%s.pm25 %s %d' % (i, airdata[i][0], timestamp),
      'air.guangzhou.%s.pm25_iaqi %s %d' % (i, airdata[i][1], timestamp),
      'air.guangzhou.%s.pm10 %s %d' % (i, airdata[i][2], timestamp),
      'air.guangzhou.%s.pm10_iaqi %s %d' % (i, airdata[i][3], timestamp),
      'air.guangzhou.%s.so2 %s %d' % (i, airdata[i][4], timestamp),
      'air.guangzhou.%s.so2_iaqi %s %d' % (i, airdata[i][5], timestamp),
      'air.guangzhou.%s.no2 %s %d' % (i, airdata[i][6], timestamp),
      'air.guangzhou.%s.no2_iaqi %s %d' % (i, airdata[i][7], timestamp),
      'air.guangzhou.%s.co %s %d' % (i, airdata[i][8], timestamp),
      'air.guangzhou.%s.co_iaqi %s %d' % (i, airdata[i][9], timestamp),
      'air.guangzhou.%s.o3 %s %d' % (i, airdata[i][10], timestamp),
      'air.guangzhou.%s.o3_iaqi %s %d' % (i, airdata[i][11], timestamp)
    ]
    message = '\n'.join(lines) + '\n'
    send_msg(message)
    # delay for graphite server will use a DELAY time for inserting data
    time.sleep(DELAY)
</code></pre><h3 id=使用方法>使用方法</h3><p>将上面的文件保存为可执行文件，然后使用crontab添加一个定时任务，譬如以下的crontab条目会
在每个小时的xx:05分时自动运行该脚本文件，将取回的数据写入到Graphite远端。</p><pre><code>$ crontab -l
# hourly execute pm25 updating task, xx:05 will be the execute time
5 */1 * * * export DISPLAY=:0;/home/adminubuntu/GuangzhouPM25.py
</code></pre><p>写入graphite后的效果如下:<br><img src=/images/2015_12_16_12_20_04_284x453.jpg alt=/images/2015_12_16_12_20_04_284x453.jpg></p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/2015/12/15/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang/>用Graphite呈现广州空气质量</a></h1><span class=post-date>Dec 15, 2015<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=数据源准备>数据源准备</h3><p>数据源地址在:<br><a href=http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html>http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html</a></p><p>但是这个地址取回数据比较困难。而在<a href=http://www.gzepb.gov.cn/>http://www.gzepb.gov.cn/</a>
右侧的栏里可以通过点击，打开某个监测点当前的空气质量指数,例如海珠湖的数据位于:</p><p><a href="http://210.72.1.216:8080/gzaqi_new/DataList2.html?EPNAME=%E6%B5%B7%E7%8F%A0%E6%B9%96">http://210.72.1.216:8080/gzaqi_new/DataList2.html?EPNAME=%E6%B5%B7%E7%8F%A0%E6%B9%96</a></p><h3 id=beautiful-soup>Beautiful Soup</h3><p>Beautiful Soup可以被理解为网页爬虫，用于爬取某个页面并取回所需信息。在Ubuntu/Debian系统
中，安装命令如下。同时为了使用对XML解析速度更快的lxml解析器，我们安装python-lxml:</p><pre><code>$ sudo apt-get install -y python-bs4
$ sudo apt-get install -y python-lxml 
</code></pre><p>现在我们打开某个终端，开始用命令行交互的方式，取回海珠湖监测点的数据:</p><p>首先，引入所需的库：</p><pre><code># python
Python 2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2] on linux2
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; from bs4 import BeautifulSoup
&gt;&gt;&gt; import urllib2
&gt;&gt;&gt; response = urllib2.urlopen('http://210.72.1.216:8080/gzaqi_new/DataList2.html?EPNAME=%E6%B5%B7%E7%8F%A0%E6%B9%96')
&gt;&gt;&gt; print response.info()
Content-Length: 10216
Content-Type: text/html
Last-Modified: Wed, 13 May 2015 08:12:28 GMT
Accept-Ranges: bytes
ETag: &quot;b680828d548dd01:da2&quot;
Server: Microsoft-IIS/6.0
X-Powered-By: ASP.NET
Date: Tue, 15 Dec 2015 02:25:17 GMT
Connection: close

&gt;&gt;&gt; html = response.read()
&gt;&gt;&gt; print &quot;Get the length :&quot;, len(html)
Get the length : 10216
&gt;&gt;&gt; response.close()  # best practice to close the file
</code></pre><p>上述的操作里调用urllib2取回了页面， html变量里包含了该网页的内容。接下来我们使用
BeautifulSoup来美化并从中取回我们想要的元素。</p><pre><code>&gt;&gt;&gt; soup = BeautifulSoup(html, 'html.parser')    
&gt;&gt;&gt; print soup.prettify()
</code></pre><p>仔细检查后发现，用urllib2取回的网页中，html变量里未包含当前的数据值。通过阅读代码得知，
当前页面的值是浏览器在载入网页时执行javascript函数得到的。因而我们使用一个真实的浏览器
来实现页面的抓取。</p><p>Selenium是一套用于进行浏览器自动化测试的开源工具集，可进行Web应用的端到端测试
。Selenium主要包括两个工具：一是Selenium IDE，二是Selenium WebDriver（简称
WebDriver）. 安装命令如下:</p><pre><code>$ pip install selenium
</code></pre><p>使用selenium抓取该网页的代码如下:</p><pre><code>&gt;&gt;&gt; from contextlib import closing
&gt;&gt;&gt; from selenium.webdriver import Firefox
&gt;&gt;&gt; from selenium.webdriver.support.ui import WebDriverWait
&gt;&gt;&gt; url='http://210.72.1.216:8080/gzaqi_new/DataList2.html?EPNAME=%E6%B5%B7%E7%8F%A0%E6%B9%96'
&gt;&gt;&gt; with closing(Firefox()) as browser:
...   browser.get(url)
...   page_source = browser.page_source
... 
&gt;&gt;&gt; print page_source
&gt;&gt;&gt; soup = BeautifulSoup(page_source, 'html.parser')
&gt;&gt;&gt; print soup
</code></pre><p>现在我们可以看到，取回的<code>page_source</code>变量中已经包含有该时段的数据。接下来就是如何把数据
从其中提取出来的过程。</p><p>定位到含有数据的表格, 根据其层叠结构，获得tr的值:</p><pre><code>&gt;&gt;&gt; table = soup.find('table', {'class': 'headTable'})
&gt;&gt;&gt; for td in table.tbody.tr:
...     print td
... 
&lt;td class=&quot;SO2_24H&quot;&gt;7&lt;/td&gt;
&lt;td class=&quot;NO2_24H&quot;&gt;50&lt;/td&gt;
&lt;td class=&quot;PM10_24H&quot;&gt;29&lt;/td&gt;
&lt;td class=&quot;CO_24H&quot;&gt;31&lt;/td&gt;
&lt;td class=&quot;O3_8H_24H&quot;&gt;18&lt;/td&gt;
&lt;td class=&quot;PM25_24H&quot;&gt;25&lt;/td&gt;
&lt;td class=&quot;AQI&quot;&gt;50&lt;/td&gt;
&lt;td class=&quot;Pollutants&quot;&gt;—&lt;/td&gt;
&lt;td class=&quot;jibie2&quot;&gt;--&lt;/td&gt;
&lt;td class=&quot;jibie2&quot;&gt;一级&lt;/td&gt;
&lt;td class=&quot;leibie&quot;&gt;优                  &lt;/td&gt;
&lt;td class=&quot;yanse&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;Images/you.jpg&quot;/&gt;&lt;/td&gt;
</code></pre><p>更进一步得到值:</p><pre><code>&gt;&gt;&gt; for td in table.tbody.tr:
...     print td.contents[0]
... 
7
50
29
31
18
25
50
—
--
一级
优                  
&lt;img alt=&quot;&quot; src=&quot;Images/you.jpg&quot;/&gt;
</code></pre><p>对应的图片如下:</p><p><img src=/images/2015_12_15_12_06_23_943x201.jpg alt=/images/2015_12_15_12_06_23_943x201.jpg></p><p>提取出来了数据，就可以做后续处理了。</p><h3 id=graphite>Graphite</h3><p>Graphite的搭建过程不提及。基于我们前面提取出的数据，只需要将其写入Graphite，就可以看
到数据的显示了。</p><p>具体的写入代码参考(需翻墙):</p><p><a href=http://coreygoldberg.blogspot.com/2012/04/python-getting-data-into-graphite-code.html>http://coreygoldberg.blogspot.com/2012/04/python-getting-data-into-graphite-code.html</a></p><p>按照博客中提供的例子，写入到Graphite后的数据在Graphite看起来是这样的:</p><p><img src=/images/2015_12_15_14_48_48_318x115.jpg alt=/images/2015_12_15_14_48_48_318x115.jpg></p><p>而对应的数据格式则如下:</p><pre><code> sending message:
     system.monitorserver.loadavg_1min 0.18 1450161396
     system.monitorserver.loadavg_5min 0.25 1450161396
     system.monitorserver.loadavg_15min 0.23 1450161396
</code></pre><p>我们可以仿照这样的数据来组织自己的空气质量数据。</p><h3 id=数据来源再加工>数据来源再加工</h3><p>前面取回地址失败， 因为它只是返回空气日报的地址，我们需要的是实时情况，所以还是回到<br><a href=http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html>http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html</a></p><p>这里需要在selenium里模拟出鼠标快速点击所有链接的效果。</p><p>下面是一次完整的点击白云山按钮并获得PM2.5页面的过程:</p><pre><code>root@monitorserver:~/Code# python
Python 2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2] on linux2
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; from contextlib import closing
&gt;&gt;&gt; from selenium.webdriver import Firefox
&gt;&gt;&gt; from selenium.webdriver.support.ui import WebDriverWait
&gt;&gt;&gt; driver = Firefox()                                                 
&gt;&gt;&gt; driver.get('http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html')
&gt;&gt;&gt; driver.refresh()
&gt;&gt;&gt; baiyunmountain=driver.find_element_by_id(&quot;白云山&quot;)
&gt;&gt;&gt; baiyunmountain.click()
&gt;&gt;&gt; PM25=driver.find_element_by_id(&quot;PM25&quot;)
&gt;&gt;&gt; type(PM25)
&lt;class 'selenium.webdriver.remote.webelement.WebElement'&gt;
&gt;&gt;&gt; PM25.click()
</code></pre></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/131/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/131/>131</a></li><li class="page-item active"><a class=page-link href=/page/132/>132</a></li><li class=page-item><a class=page-link href=/page/133/>133</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/245/>245</a></li><li class=page-item><a href=/page/133/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/245/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>