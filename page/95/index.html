<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2016/12/23/loadbalancingincoreos/>LoadBalancingInCoreOS</a></h1><span class=post-date>Dec 23, 2016<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>这几天一直在把玩CoreOS，
主要参考的是DigitalOcean上的tutorial以及《CoreOS实践之路》这本书，
奈何文章离如今年代已经久远，一直搭建不成功. 几经周折后终于在一篇guideline
的指导下把负载均衡的服务跑通，这里是搭建该服务的步骤和心得。</p><p>参考网址:</p><p><a href=http://blog.dixo.net/2015/02/load-balancing-with-coreos/>http://blog.dixo.net/2015/02/load-balancing-with-coreos/</a></p><h3 id=架构图>架构图</h3><p>该网址中列举的的架构图如下:</p><p><img src=/images/2016_12_23_19_10_52_1450x1029.jpg alt=/images/2016_12_23_19_10_52_1450x1029.jpg></p><p>刚开始看到这个图是有点发蒙的，这里简单说一下操作步骤，与图中一一对应.</p><ol><li><code>CoreOS Machine B</code>和<code>Core Machine C</code>是两个CoreOS系统节点，位于其上分别
运行了两个apache容器，B上的容器监听8001端口，C上的容器监听8002端口。</li><li>两个apache容器将自身的IP和端口发布到etcd服务(etcd.service).</li><li><code>CoreOS Machine A</code>上运行了三个单元，分别是nginx服务、confdata服务、confd服务.
其中nginx服务在配置好的负载均衡后端上分发http请求。confdata服务主要用于
为nginx配置文件共享数据卷。confd服务查看etcd中元数据的变化，根据这些变化
在共享数据卷中写入新的配置文件.</li><li>详细说明一下confd的作用，A. 发现coreos集群中可用的apache容器. B. 实时生成
nginx.conf文件，并将此文件写入到共享存储. C. 写入完成后，通知docker给nginx发送
一个HUP信号. D. Docker发送HUP信号给nginx容器后，容器将重新加载其配置文件。</li></ol><p>以上就是我对架构图的解读。接下来将一步步来实现这个负载均衡。</p><h3 id=先决条件>先决条件</h3><p>一个3节点的CoreOS集群<br>有效的/etc/environment文件(有时候需要手动生成)
相关的容器(制作流程见后)</p><h3 id=容器镜像制作>容器镜像制作</h3><h4 id=apache容器>Apache容器</h4><p>在某台安装好Docker的机器上，或者直接在CoreOS节点机上，运行:</p><pre><code>$ docker run -i -t ubuntu:14.04 /bin/bash
# apt-get update
# apt-get install apache2
# sudo bash
# echo &quot;&lt;h1&gt;Running from Docker on CoreOS&lt;/h1&gt;&quot; &gt; /var/www/html/index.html
# exit
</code></pre><p>现在开始打包容器:</p><pre><code>$ docker ps -l
$ docker commit  container_ID dash/apache
$ docker save dash/apache&gt;myapache.tar
</code></pre><p>将生成的tar文件分发到所有CoreOS节点上，使用<code>docker load&lt;./myapache.tar</code>加载之.</p><h4 id=nginx容器>nginx容器</h4><p>直接使用官方的nginx:latest即可</p><h4 id=nginx-lb容器>nginx-lb容器</h4><p>使用Dockerfile生成:</p><pre><code>$ vim Dockerfile
FROM ubuntu:14.04

RUN apt-get update &amp;&amp; \
    DEBIAN_FRONTEND=noninteractive apt-get -y install curl &amp;&amp; \
    curl -o /usr/bin/confd -L https://github.com/kelseyhightower/confd/releases/download/v0.7.1/confd-0.7.1-linux-amd64 &amp;&amp; \
    chmod 755 /usr/bin/confd  &amp;&amp; curl -sSL http://acs-public-mirror.oss-cn-hangzhou.aliyuncs.com/docker-engine/internet | sh -

ADD etc/confd/ /etc/confd

CMD /usr/bin/confd -interval=60 -node=http://$COREOS_PRIVATE_IPV4:4001
</code></pre><p>编译方法为<code>sudo docker build -t myconfd .</code></p><p>而后的打包和解包方法同上。奇怪的是，我制作的镜像到最后居然无法启动，所以直接pull
了作者制作的镜像:</p><pre><code>$ sudo docker pull lordelph/confd-demo
</code></pre><h3 id=创建apache服务>创建apache服务</h3><p>创建apache.service配置文件如下:</p><pre><code>$ vim apache.service 
[Unit]
Description=Basic web service port %i
After=docker.service
Requires=docker.service

[Service]
EnvironmentFile=/etc/environment
ExecStartPre=-/usr/bin/docker kill apache-%i
ExecStartPre=-/usr/bin/docker rm apache-%i
ExecStartPre=/usr/bin/etcdctl set /test/apache-%i ${COREOS_PRIVATE_IPV4}:%i
ExecStart=/usr/bin/docker run --rm --name apache-%i -p ${COREOS_PRIVATE_IPV4}:%i:80 dash/apache  /usr/sbin/apache2ctl -D FOREGROUND
ExecStop=/usr/bin/etcdctl rm /test/apache-%i
ExecStop=/usr/bin/docker stop -t 3 apache-%i
</code></pre><p>这里要说明的几点是: 在创建该服务前将清空该节点上所有的同名容器运行历史，并在etcd中设置
一个键值为<code>/test/apache-%i</code>的数据条目。关闭该服务则停止容器的运行，并删除该键值.</p><p><code>COREOS_PRIVATE_IPV4</code>这个值由<code>/etc/environment</code>中给出，因而事先需要
在每个节点上核查是否存在该值:</p><pre><code>core@coreos1 ~/lb $ cat /etc/environment 
COREOS_PUBLIC_IPV4=172.17.8.201
COREOS_PRIVATE_IPV4=172.17.8.201
</code></pre><p><code>apache-%i</code>里的端口参数可以在启动时给定, %i占位符将被@后的值所代替。创建服务步骤如下:</p><pre><code>$ ln -s apache.service apache@8001.service 
$ ln -s apache.service apache@8002.service 
$ fleetctl start apache@8001.service
$ fleetctl start apache@8002.service
</code></pre><p>查看运行的服务单元:</p><pre><code>$ fleetctl list-units
UNIT			MACHINE				ACTIVE	SUB
apache@8001.service	bea5741d.../172.17.8.203	active	running
apache@8002.service	dd464e69.../172.17.8.202	active	running
</code></pre><p>检查etcd中新添加的值:</p><pre><code>$ etcdctl ls /test
/test/apache-8001
/test/apache-8002
</code></pre><p>key对应的value:</p><pre><code>$ etcdctl get /test/apache-8001
172.17.8.203:8001
</code></pre><h3 id=confd数据卷>confd数据卷</h3><p>用于创建confd数据卷的service定义文件如下:</p><pre><code>$ vim confdata.service 
[Unit]
Description=Configuration Data Volume Service
After=docker.service
Requires=docker.service

[Service]
EnvironmentFile=/etc/environment

#we aren't a normal service, we just need to ensure that a data volume
#exists, and create one if it doesn't
Type=oneshot
RemainAfterExit=yes

ExecStartPre=-/usr/bin/docker rm conf-data
ExecStart=/usr/bin/docker run -v /etc/nginx --name conf-data nginx echo &quot;created new data container&quot;
</code></pre><p>这里的技巧在于<code>oneshot</code>属性，它告知systemd我们只希望该服务运行且只运行一次。<code>RemainAfterExit</code>告诉
服务正常退出，因而我们可以在正常推出的服务上使用它创建的数据卷。</p><p>运行该服务:</p><pre><code>$ fleetctl start confdata.service
$ fleetctl list-units
UNIT			MACHINE				ACTIVE	SUB
apache@8001.service	bea5741d.../172.17.8.203	active	running
apache@8002.service	dd464e69.../172.17.8.202	active	running
confdata.service	f22aee5d.../172.17.8.201	active	exited
</code></pre><p>可以使用以下一系列命令检查我们生成的数据卷, 注意要使用root才能看到该卷里的实际内容:</p><pre><code>$ docker volume ls
DRIVER              VOLUME NAME
local               c19a35d3db97340272f5d191b166710f6fbb1d717225d9762417fa51c7a56b1f
core@coreos1 ~/lb $ docker volume inspect c19a35d3db97340272f5d191b166710f6fbb1d717225d9762417fa51c7a56b1f
[
    {
        &quot;Name&quot;: &quot;c19a35d3db97340272f5d191b166710f6fbb1d717225d9762417fa51c7a56b1f&quot;,
        &quot;Driver&quot;: &quot;local&quot;,
        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/c19a35d3db97340272f5d191b166710f6fbb1d717225d9762417fa51c7a56b1f/_data&quot;,
        &quot;Labels&quot;: null
    }
]
core@coreos1 ~/lb $ sudo bash
coreos1 lb # cd /var/lib/docker/volumes/c19a35d3db97340272f5d191b166710f6fbb1d717225d9762417fa51c7a56b1f/_data
coreos1 _data # ls
conf.d  fastcgi_params  koi-utf  koi-win  mime.types  modules  nginx.conf  scgi_params  uwsgi_params  win-utf
</code></pre><h3 id=confd服务>confd服务</h3><p>confd.service文件定义如下:</p><pre><code>$ vim confd.service 
[Unit]
Description=Configuration Service

#our data volume must be ready
After=confdata.service
Requires=confdata.service


[Service]
EnvironmentFile=/etc/environment


#kill any existing confd
ExecStartPre=-/usr/bin/docker kill %n
ExecStartPre=-/usr/bin/docker rm %n

#preload container...this ensures we fail if our registry is down and we can't
#obtain the build we're expecting

#we need to provide our confd container with the IP it can reach etcd
#on, the docker socket so it send HUP signals to nginx, and our data volume
ExecStart=/usr/bin/docker run --rm \
  -e COREOS_PRIVATE_IPV4=${COREOS_PRIVATE_IPV4} \
  -v /var/run/docker.sock:/var/run/docker.sock \
  --volumes-from=conf-data \
  --name %n \
  lordelph/confd-demo

ExecStop=/usr/bin/docker stop -t 3 %n
Restart=on-failure

[X-Fleet]
#we need to be on the same machine as confdata.service
MachineOf=confdata.service
</code></pre><p>注意，我们这里直接使用了网页中作者制作的镜像，指定了<code>conf-data</code>数据卷(<code>--volumes-from=conf-data</code>).</p><pre><code>$ fleetctl list-units
UNIT			MACHINE				ACTIVE	SUB
apache@8001.service	bea5741d.../172.17.8.203	active	running
apache@8002.service	dd464e69.../172.17.8.202	active	running
confd.service		f22aee5d.../172.17.8.201	active	running
confdata.service	f22aee5d.../172.17.8.201	active	exited
</code></pre><p>检查配置文件是否被更新:</p><pre><code>coreos1 _data # grep -A6 'upstream backend' ./nginx.conf 
    upstream backend {
        
            server 172.17.8.202:8002;
        
            server 172.17.8.203:8001;
        
    }
coreos1 _data # pwd
/var/lib/docker/volumes/c19a35d3db97340272f5d191b166710f6fbb1d717225d9762417fa51c7a56b1f/_data
</code></pre><p>或者直接进入容器检查:</p><pre><code># docker run --rm -ti --volumes-from=conf-data nginx \
# grep -A6 'upstream backend' /etc/nginx/nginx.conf
    upstream backend {
        
            server 172.17.8.101:8001;
        
            server 172.17.8.102:8002;
        
    }
</code></pre><h3 id=nginx服务>nginx服务</h3><p>这个服务很简单，定义文件如下:</p><pre><code>$ vim nginx.service 
[Unit]
Description=Nginx Service
After=confd.service

#we won't want it to require the service - that would stop us restarting
#it, which is safe
#Requires=confd.service

[Service]
EnvironmentFile=/etc/environment
ExecStartPre=-/usr/bin/docker kill %n
ExecStartPre=-/usr/bin/docker rm %n
#ExecStartPre=/usr/bin/docker pull nginx
ExecStart=/usr/bin/docker run --name %n -p 80:80 --volumes-from=conf-data nginx
ExecStop=/usr/bin/docker stop -t 3 %n
Restart=on-failure

[X-Fleet]
#we need to be on the same machine as confdata
MachineOf=confdata.service
</code></pre><p>这个服务被定义为只能在运行了confdata.service的机器上运行，实际上因为我们为了使用
<code>conf-data</code>数据卷。<br>启动并检查服务:</p><pre><code>$ fleetctl start nginx.service
$ fleetctl list-units
UNIT			MACHINE				ACTIVE	SUB
apache@8001.service	bea5741d.../172.17.8.203	active	running
apache@8002.service	dd464e69.../172.17.8.202	active	running
confd.service		f22aee5d.../172.17.8.201	active	running
confdata.service	f22aee5d.../172.17.8.201	active	exited
nginx.service		f22aee5d.../172.17.8.201	active	running
</code></pre><h3 id=测试>测试</h3><p>打开浏览器，访问<code>172.17.8.201</code>，即可看到apache服务的页面。<br>可以使用tcpdump来查看流量走向，然而需要安装toolbox: <code>/usr/bin/toolbox</code>.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2016/12/22/coreostips/>CoreOSTips</a></h1><span class=post-date>Dec 22, 2016<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=add-additional-ssh-keys>Add additional ssh keys</h3><p>Adding new keys into the deployed system:</p><pre><code># echo 'ssh-rsa AAAAB3Nza.......  key@host' | update-ssh-keys -a core
</code></pre><h3 id=write-files>Write files</h3><p>Take <code>/etc/environment</code> file for example:</p><pre><code>core@coreos1 ~ $ cat /usr/share/oem/cloud-config.yml 
#cloud-config
write_files:
    - path: /etc/environment
      permissions: 0644
      content: |
        COREOS_PUBLIC_IPV4=172.17.8.201
        COREOS_PRIVATE_IPV4=172.17.8.201
</code></pre><h3 id=add-user>Add User</h3><p>Also add in the file <code>/usr/share/oem/cloud-config.yml</code>, like following:</p><pre><code>users:
  - name: &quot;dash&quot;
    passwd: &quot;xxxxxxxxxxxxxxxxxx&quot;
    groups:
      - &quot;sudo&quot;
      - &quot;docker&quot;
    ssh-authorized-keys:
      - &quot;ssh-rsa ADD ME&quot;
</code></pre><p>Password could be generated via <code>openssl -1 "YourPasswd"</code></p><h3 id=use-ansible>Use Ansible</h3><p>Install via:</p><pre><code>$ sudo ansible-galaxy install defunctzombie.coreos-bootstrap
$ sudo vim /etc/ansible/roles/defunctzombie.coreos-bootstrap/files/bootstrap.sh
if [[ -e $HOME/pypy-5.6-linux_x86_64-portable.tar.bz2 ]]; then
  tar -xjf $HOME/pypy-5.6-linux_x86_64-portable.tar.bz2
  #rm -rf $HOME/pypy-$PYPY_VERSION-linux64.tar.bz2
else
  wget -O - https://bitbucket.org/pypy/pypy/downloads/pypy-$PYPY_VERSION-linux64.tar.bz2 |tar -xjf -
fi

rm -rf pypy
mv -n pypy-5.6-linux_x86_64-portable pypy
</code></pre><p>Cause the old version 5.1.0 will have problems, we replace it with 5.6.</p><p>Create playbook and inventory file:</p><pre><code>$ vim site.yml 
- hosts: coreos
  gather_facts: False
  roles:
    - defunctzombie.coreos-bootstrap
$ vim inventory
[coreos]
172.17.8.221
172.17.8.222
172.17.8.223

[coreos:vars]
ansible_ssh_user=core
ansible_python_interpreter=/home/core/bin/python
$ ansible-playbook -i inventory site.yml
$ ansible -i inventory all -m ping
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2016/12/21/trycoreos3/>TryCoreOS(3)</a></h1><span class=post-date>Dec 21, 2016<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=fleetctl-configuration>fleetctl Configuration</h3><h4 id=cluster-status>Cluster Status</h4><p><code>fleetctl list-machines</code> will display all of the nodes in cluster:</p><pre><code>core@coreos1 ~ $ fleetctl list-machines
MACHINE		IP		METADATA
bea5741d...	172.17.8.203	-
dd464e69...	172.17.8.202	-
f22aee5d...	172.17.8.201	-
</code></pre><p><code>fleetctl list-units</code> will list all of the services in cluster:</p><pre><code>core@coreos1 ~ $ fleetctl list-units
UNIT	MACHINE	ACTIVE	SUB
</code></pre><h4 id=nodes-jumping>Nodes Jumping</h4><p>Use <code>ssh-keygen</code> for generating the <code>id_rsa.pub</code>, and add them into other
nodes&rsquo;s <code>/home/core/.ssh/authorized_keys</code>.</p><p>Start the ssh-agent via:</p><pre><code>$ eval `ssh-agent`
$ ssh-add /home/core/.ssh/id_rsa
$ fleetctl ssh dd464e69
</code></pre><p>List the added ssh key via:</p><pre><code>$ ssh-add  -l
2048 SHA256:w7OM8b6ximc9/lTgaB5gWpHK6xuf22IE37Of113yfJA /home/core/.ssh/id_rsa (RSA)
</code></pre><p>Then you could use <code>fleetctl ssh dd464e69</code> for jumping to <code>172.17.8.202</code>.</p><p>Or execute command like following:</p><pre><code> $ fleetctl ssh dd464e69 cat /etc/hostname
coreos2
</code></pre><h3 id=start-first-fleet-unit>Start First Fleet Unit</h3><p>In core1 node, do following operation:</p><pre><code>$ cp /etc/systemd/system/hello.service ./
$ vim hello.service 
$ fleetctl start ./hello.service 
</code></pre><p>Your hello.service should be like this:</p><pre><code>[Unit] 
Description=Hello World 
After=docker.service 
Requires=docker.service 

[Service] 
TimeoutStartSec=0 
ExecStartPre=-/bin/docker kill busybox1 
ExecStartPre=-/bin/docker rm busybox1 
ExecStart=/bin/docker run --name busybox1 busybox /bin/sh -c &quot;while true; do
echo Hello World; sleep 1; done&quot; 
ExecStop=&quot;/bin/docker kill busybox1&quot;

[X-Fleet]
X-Conflicts=hello*.service
</code></pre><p>Mainly the same as systemd&rsquo;s configuration, but replace the last part to
<code>X-Fleet</code>.</p><p>Start the unit via:</p><pre><code>core@coreos1 ~ $ fleetctl start ./hello.service 
Unit hello.service inactive
Unit hello.service launched on bea5741d.../172.17.8.203
core@coreos1 ~ $ fleetctl list-units
UNIT		MACHINE				ACTIVE	SUB
hello.service	bea5741d.../172.17.8.203	active	running
</code></pre><p>In node3(172.17.8.203), you could use <code>systemctl list-units</code> for finding the
service named <code>hello.service</code>.</p><h3 id=service-migration>Service Migration</h3><p>Step1: Login to hello.service machine.<br>Step2: Reboot this node.<br>Step3: View current units has been migrated to new node.<br>Step4: List the avaiable machines.</p><pre><code>core@coreos1 ~ $ fleetctl ssh hello.service
Last login: Wed Dec 21 10:24:15 UTC 2016 from 172.17.8.201 on pts/1
CoreOS stable (1185.5.0)
core@coreos3 ~ $ systemctl reboot
core@coreos3 ~ $ core@coreos1 ~ $ 
core@coreos1 ~ $ fleetctl list-units
UNIT		MACHINE				ACTIVE	SUB
hello.service	dd464e69.../172.17.8.202	active	running
core@coreos1 ~ $ fleetctl list-machines
MACHINE		IP		METADATA
dd464e69...	172.17.8.202	-
f22aee5d...	172.17.8.201	-
</code></pre><h3 id=destroy-service>Destroy Service</h3><p>If you want to destroy the service, do following:</p><pre><code>core@coreos1 ~ $ fleetctl list-unit-files
UNIT		HASH	DSTATE		STATE		TARGET
hello.service	09bb151	launched	launched
dd464e69.../172.17.8.202
core@coreos1 ~ $ fleetctl destroy hello.service
Destroyed hello.service
core@coreos1 ~ $ fleetctl list-unit-files
UNIT	HASH	DSTATE	STATE	TARGET
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2016/12/21/trycoreos2/>TryCoreOS(2)</a></h1><span class=post-date>Dec 21, 2016<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=local-discovery-service>Local Discovery Service</h3><p>Take a look at the yaml configuration file:</p><pre><code>  etcd2:
    # generate a new token for each unique cluster from
https://discovery.etcd.io/new?size=3
    # specify the initial size of your cluster with ?size=X
    discovery: https://discovery.etcd.io/4add2186302763c8876afd1684ca06fe
</code></pre><p>This means all of you coreos nodes should reach the internet, what if we
deploy a coreos cluster offline? We need to deploy a local discovery service.</p><h4 id=archlinux-etcd2-example>ArchLinux etcd2 Example</h4><p>Download the etcd v2.3.7 and extract it to specified directory via :</p><pre><code>$ curl -L \
https://github.com/coreos/etcd/releases/download/v2.3.7/etcd-v2.3.7-linux-amd64.tar.gz \
-o etcd-v2.3.7-linux-amd64.tar.gz
$ tar xzvf etcd-v2.3.7-linux-amd64.tar.gz
</code></pre><p>Create a customized systemd item like following, you should change the etcd
executable file to your own positioni, also specify your own <code>data-dir</code>:</p><pre><code>$ sudo mkdir -p /var1/Nov14/etcd/etcd-v2.3.7-linux-amd64/data
$ sudo vim /usr/lib/systemd/system/myetcd.service 
[Unit]
Description=myownetcd
After=multi-user.service

[Service]
ExecStart=/var1/Nov14/etcd/etcd-v2.3.7-linux-amd64/etcd
--data-dir=/var1/Nov14/etcd/etcd-v2.3.7-linux-amd64/data
--name=single-etcd-service  --listen-client-urls
'http://0.0.0.0:2379,http://0.0.0.0:4001' --advertise-client-urls
'http://0.0.0.0:2379,http://0.0.0.0:4001'
Restart=always
StandardOutput=syslog
StandardError=syslog
SyslogIdentifier=myownetcd

[Install]
WantedBy=multi-user.target
</code></pre><p>Start and enable the service:</p><pre><code>$ sudo systemctl enable myetcd.service
$ sudo systemctl start myetcd.service
</code></pre><h4 id=test-etcd>Test etcd</h4><p>Very simple test:</p><pre><code>$ ./etcdctl set mykey &quot;this is awesome&quot;
$ ./etcdctl get mykey
this is awesome
</code></pre><h4 id=create-own-discovery>Create own discovery</h4><p>Use <code>uuidgen</code> for generating a new uuid:</p><pre><code>$ uuidgen 
557a2133-88f3-41d2-9b27-9fd4081f7b41
</code></pre><p>Use this new generated uuid for generating a 3-node cluster:</p><pre><code>$ curl -X PUT \
http://172.17.8.1:4001/v2/keys/557a2133-88f3-41d2-9b27-9fd4081f7b41/_config/size \
-d value=3
</code></pre><p>Now you could examine the generated etcd items:</p><pre><code>$ /var1/Nov14/etcd/etcd-v2.3.7-linux-amd64/etcdctl ls --recursive /
/mykey
/557a2133-88f3-41d2-9b27-9fd4081f7b41
</code></pre><h4 id=use-your-own-discovery>Use your own discovery</h4><p>Change the definition in yaml file:</p><pre><code>    #discovery: https://discovery.etcd.io/4add2186302763c8876afd1684ca06fe
    discovery: http://172.17.8.1:4001/v2/keys/557a2133-88f3-41d2-9b27-9fd4081f7b41
</code></pre><p>Now re-create the coreOS cluster, you will see the coreOS cluster could
work properly again.</p><h3 id=start-first-docker>Start First Docker</h3><p>Enable registry-mirror in coreOS:</p><pre><code># echo 'DOCKER_OPTS=&quot;--registry-mirror=http://1a653205.m.daocloud.io&quot;' &gt;&gt; /run/flannel_docker_opts.env
# systemctl restart docker
# docker pull busybox
</code></pre><p>Now write the hello.service definition:</p><pre><code># vim /etc/systemd/system/hello.service
[Unit] 
Description=Hello World 
After=docker.service 
Requires=docker.service 

[Service] 
TimeoutStartSec=0 
ExecStartPre=-/bin/docker kill busybox1 
ExecStartPre=-/bin/docker rm busybox1 
ExecStart=/bin/docker run --name busybox1 busybox /bin/sh -c &quot;while true; do echo Hello World; sleep 1; done&quot; 
ExecStop=&quot;/bin/docker kill busybox1&quot;
ExecStopPost=&quot;/bin/docker rm busybox1&quot;
 
[Install] 
WantedBy=multi-user.target
</code></pre><p>Start and enable the service via:</p><pre><code># systemctl start hello.service
# systemctl enable hello.service
</code></pre><h3 id=trouble-shooting>Trouble-Shooting</h3><p>Reset your etcd2 database:</p><pre><code>$ sudo systemctl stop fleetd
$ sudo systemctl stop etcd2
$ sudo rm -rf /var/lib/etcd2/*
$ sudo rm -rf /etc/systemd/system/etcd* 
$ sudo reboot 
</code></pre><p>Then your etcd2 will be re-initialized.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2016/12/20/trycoreos/>TryCoreOS</a></h1><span class=post-date>Dec 20, 2016<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=preparation>Preparation</h3><p>Create image file via:</p><pre><code>$ sudo mkdir corecluster
$ cd corecluster
$ qemu-img create -f qcow2 coreos1.qcow2 30G
$ qemu-img create -f qcow2 coreos2.qcow2 30G
$ qemu-img create -f qcow2 coreos3.qcow2 30G
</code></pre><p>Create a network named <code>172.17.8.1/24</code>, dhcp disabled.</p><p>Since the vnet interface is occupied via virtualbox, switches to virtualbox
installation.</p><p>Two Ethernet cards:</p><p><img src=/images/2016_12_20_17_36_44_697x413.jpg alt=/images/2016_12_20_17_36_44_697x413.jpg></p><p>Start from CD:</p><p><img src=/images/2016_12_20_17_37_26_678x304.jpg alt=/images/2016_12_20_17_37_26_678x304.jpg></p><h3 id=installation-file-preparation>Installation File Preparation</h3><p>For saving time, we use local installation repository, download the
installation images via:</p><pre><code>$ $ git clone https://github.com/coreos/coreos-baremetal
# Make a copy of example files
$ cp -R coreos-baremetal/examples .
# Download the CoreOS image assets referenced in the target profile.
$ ./coreos-baremetal/scripts/get-coreos stable 1185.5.0 ./examples/assets
</code></pre><p>Then Copy installation image and sig file into the webserver:</p><pre><code>$ cd /YourWebServerDirectory/1185.5.0 
$ ls
coreos_production_image.bin.bz2  coreos_production_image.bin.bz2.sig
</code></pre><h3 id=yaml-definition>YAML Definition</h3><p>Refers to <code>coreos-vagrant</code> project:</p><pre><code>$ git clone https://github.com/coreos/coreos-vagrant.git`
$ cat user-data
</code></pre><p>The configuration of the yaml is listed as following:</p><p>First get the token via:</p><pre><code>$ curl https://discovery.etcd.io/new?size=3
https://discovery.etcd.io/xxxxxxxxxxxxxxxxxxxxxxxxx
</code></pre><pre><code>#cloud-config
hostname: coreos1 
coreos:
  etcd2:
    # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3
    # specify the initial size of your cluster with ?size=X
    discovery: https://discovery.etcd.io/xxxxxxxxxxxxxxxxxxx
    advertise-client-urls: http://172.17.8.201:2379,http://172.17.8.201:4001
    initial-advertise-peer-urls: http://172.17.8.201:2380
    # listen on both the official ports and the legacy ports
    # legacy ports can be omitted if your application doesn't depend on them
    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
    listen-peer-urls: http://172.17.8.201:2380,http://172.17.8.201:7001
  fleet:
    public-ip: &quot;172.17.8.201&quot;
  flannel:
    interface: &quot;172.17.8.201&quot;
  units:
    - name: etcd2.service
      command: start
    - name: fleet.service
      command: start
    - name: flanneld.service
      drop-ins:
      - name: 50-network-config.conf
        content: |
          [Service]
          ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ &quot;Network&quot;: &quot;10.1.0.0/16&quot; }'
      command: start
    - name: static.network
      content: |
        [Match]
        Name=enp0s8
        [Network]
        Address=172.17.8.201/24
        Gateway=172.17.8.1
        DNS=172.17.8.1
    - name: docker-tcp.socket
      command: start
      enable: true
      content: |
        [Unit]
        Description=Docker Socket for the API
  
        [Socket]
        ListenStream=2375
        Service=docker.service
        BindIPv6Only=both
  
        [Install]
        WantedBy=sockets.target
users:  
  - name: core
    ssh-authorized-keys: 
      - ssh-rsa &quot;ADD ME&quot;
  - groups:
      - sudo
      - docker
</code></pre><p>For coreos2 node, simply replace the ip address from <code>172.17.8.201</code> to <code>172.17.8.202</code>, etc.</p><h3 id=installation>Installation</h3><p>Install it via:</p><pre><code># coreos-install -d /dev/sda -b http://YourWebServer -c ./YourYamlFile.yaml -v
</code></pre><p>After a while, the installation will finish.</p><p>Repeat this step in 3 nodes.</p><h3 id=verification>Verification</h3><p>Login into any of the core machine in cluster:</p><pre><code>core@coreos1 ~ $ etcdctl cluster-health
member f934a5ba1eca1ea is healthy: got healthy result from http://172.17.8.201:2379
member 23798f79754d53b7 is healthy: got healthy result from http://172.17.8.203:2379
member 5acdd34e67ade1d7 is healthy: got healthy result from http://172.17.8.202:2379
cluster is healthy
</code></pre></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/94/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/94/>94</a></li><li class="page-item active"><a class=page-link href=/page/95/>95</a></li><li class=page-item><a class=page-link href=/page/96/>96</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/231/>231</a></li><li class=page-item><a href=/page/96/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/231/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>