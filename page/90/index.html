<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2017/08/01/tipsonha/>TipsOnHA</a></h1><span class=post-date>Aug 1, 2017<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=network-preparation>Network Preparation</h3><p>libvirt network preparation:</p><pre><code>$ cat internal.xml
&lt;network&gt;
	&lt;name&gt;internal&lt;/name&gt;
	&lt;bridge name='virbr8'/&gt;
&lt;/network&gt;
$ cat external.xml
&lt;network&gt;
	&lt;name&gt;external&lt;/name&gt;
	&lt;bridge name='virbr9'/&gt;
&lt;/network&gt;
$ cat management.xml
&lt;network&gt;
	&lt;name&gt;management&lt;/name&gt;
	&lt;bridge name='virbr7'/&gt;
	&lt;ip address='192.168.3.1' netmask='255.255.255.0'&gt;
	&lt;/ip&gt;
&lt;/network&gt;
$ cat heartbeat.xml
&lt;network&gt;
	&lt;name&gt;heartbeat&lt;/name&gt;
	&lt;bridge name='virbr6'/&gt;
&lt;/network&gt;
</code></pre><p>Define all of the networking, take heartbeat networking for example:</p><pre><code>$ sudo virsh net-define heartbeat.xml
$ sudo virsh net-autostart heartbeat
$ sudo virsh net-start heartbeat
</code></pre><h3 id=iscsi-node>iscsi node</h3><p>Create a new machine(192.168.122.200), CentOS6.9, use local iso for
installation:</p><p>First you have to add one network card(192.168.3.200), and disable selinux,
then you do following steps:</p><pre><code># yum install -y scsi-target-utils
# mkdir -p /var/lib/tgtd/cluster01
# cd /var/lib/tgtd/cluster01/
# dd if=/dev/zero of=volume01.img bs=1M count=100
# dd if=/dev/zero of=volume02.img bs=1M count=1000
</code></pre><p>Edit the tgtd configuration:</p><pre><code># vim /etc/tgt/targets.conf
&lt;target iqn.2011-10.com.example.kvmhost01:tgt01&gt;
    backing-store /var/lib/tgtd/cluster01/volume01.img
    backing-store /var/lib/tgtd/cluster01/volume02.img
&lt;/target&gt;
# chkconfig tgtd on
# service tgtd start
# tgt-admin -s
# chkconfig iptables off
# service iptables stop
</code></pre><h3 id=node01node02>node01/node02</h3><p>Take node01 for example:</p><pre><code>[root@node01 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth1 
DEVICE=eth1
TYPE=Ethernet
ONBOOT=yes
NM_CONTROLLED=no
BOOTPROTO=static
IPADDR=192.168.3.201
NETMASK=255.255.255.0

[root@node01 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth2
DEVICE=eth2
TYPE=Ethernet
ONBOOT=yes
NM_CONTROLLED=no
BOOTPROTO=static
IPADDR=192.168.4.201
NETMASK=255.255.255.0
</code></pre><p>node02 for example:</p><pre><code>[root@node02 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth1 
DEVICE=eth1
TYPE=Ethernet
ONBOOT=yes
NM_CONTROLLED=no
BOOTPROTO=static
IPADDR=192.168.3.202
NETMASK=255.255.255.0

[root@node02 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth2
DEVICE=eth2
TYPE=Ethernet
ONBOOT=yes
NM_CONTROLLED=no
BOOTPROTO=static
IPADDR=192.168.4.202
NETMASK=255.255.255.0
</code></pre><p>Define its <code>/etc/hosts</code>:</p><pre><code>127.0.0.1   localhost
192.168.122.201	node01
192.168.122.202	node02
192.168.3.201	node01m
192.168.3.202	node02m
192.168.4.201	node01h
192.168.4.202	node02h
</code></pre><p>Also disable iptables.</p><p>ssh-keygen for ssh key-pairs and let them login without password:</p><pre><code># ssh-keygen -N &quot;&quot;
# ssh-copy-id node01
# ssh-copy-id node02
</code></pre><h3 id=find-iscsi>Find iscsi</h3><p>In node01/node02, do following:</p><pre><code># yum install -y iscsi-initiator-utils
# chkconfig iscsi on
# iscsiadm -m discovery --type sendtargets --portal 192.168.3.200
# service iscsi start
</code></pre><p>The newly added disk are named as <code>/dev/sda</code>, <code>/dev/sdb</code>.</p><h3 id=ha-add-on>HA Add-On</h3><p>In node01/node02, install the package group via:</p><pre><code># yum groupinstall -y &quot;High Availability&quot;
</code></pre><p>Start ricci service, and set the service status for cman and rgmanager:</p><pre><code># chkconfig ricci on; service ricci start
# passwd ricci
# chkconfig cman off; chkconfig rgmanager off
</code></pre><p>Install httpd in both node:</p><pre><code># yum install -y httpd
</code></pre><h3 id=node01>Node01</h3><p>Quorum Disk:</p><pre><code>[root@node01 ~]# mkqdisk -c /dev/sda -l qdisk01
mkqdisk v3.0.12.1

Writing new quorum disk label 'qdisk01' to /dev/sda.
WARNING: About to destroy all data on /dev/sda; proceed [N/y] ? y
Initializing status block for node 1...
Initializing status block for node 2...
Initializing status block for node 3...
Initializing status block for node 4...
Initializing status block for node 5...
Initializing status block for node 6...
Initializing status block for node 7...
Initializing status block for node 8...
Initializing status block for node 9...
Initializing status block for node 10...
Initializing status block for node 11...
Initializing status block for node 12...
Initializing status block for node 13...
Initializing status block for node 14...
Initializing status block for node 15...
Initializing status block for node 16...
</code></pre><p>Then format the <code>/dev/sdb</code>, and use this filesystem for saving the apache
content:</p><pre><code># mkfs.ext4 /dev/sdb
# mount /dev/sdb /mnt
# cp -ar /var/www/* /mnt/
# umount /mnt
</code></pre><h3 id=cluster-configuration>Cluster Configuration</h3><p><code>/etc/cluster/cluster.conf</code>:</p><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;cluster config_version=&quot;1&quot; name=&quot;cluster01&quot;&gt;
  &lt;cman expected_votes=&quot;3&quot;/&gt;

  &lt;clusternodes&gt;
    &lt;clusternode name=&quot;node01h&quot; nodeid=&quot;1&quot; votes=&quot;1&quot;&gt;
      &lt;fence&gt;   
        &lt;method name=&quot;virsh_reboot&quot;&gt;
          &lt;device name=&quot;kvmhost01&quot; port=&quot;node1&quot;/&gt;
        &lt;/method&gt;     
      &lt;/fence&gt;
    &lt;/clusternode&gt;

    &lt;clusternode name=&quot;node02h&quot; nodeid=&quot;2&quot; votes=&quot;1&quot;&gt;
      &lt;fence&gt;   
        &lt;method name=&quot;virsh_reboot&quot;&gt;
          &lt;device name=&quot;kvmhost01&quot; port=&quot;node2&quot;/&gt;
        &lt;/method&gt;     
      &lt;/fence&gt;
    &lt;/clusternode&gt;
  &lt;/clusternodes&gt;

  &lt;totem token=&quot;20000&quot;/&gt;
  &lt;quorumd interval=&quot;1&quot; label=&quot;qdisk01&quot; master_wins=&quot;1&quot; tko=&quot;10&quot; votes=&quot;1&quot;/&gt;

  &lt;fencedevices&gt;
    &lt;fencedevice name=&quot;kvmhost01&quot; agent=&quot;fence_virsh&quot; ipaddr=&quot;192.168.3.1&quot; login=&quot;root&quot; passwd=&quot;gwoguwoguoeg&quot; option=&quot;reboot&quot;/&gt;
  &lt;/fencedevices&gt;

  &lt;rm&gt;
    &lt;failoverdomains&gt;
      &lt;failoverdomain name=&quot;dom01&quot;&gt;
        &lt;failoverdomainnode name=&quot;node01h&quot;/&gt;
        &lt;failoverdomainnode name=&quot;node02h&quot;/&gt;
      &lt;/failoverdomain&gt;
    &lt;/failoverdomains&gt;

    &lt;service autostart=&quot;0&quot; domain=&quot;dom01&quot; name=&quot;service01&quot;&gt;
      &lt;ip address=&quot;192.168.122.209&quot; monitor_link=&quot;on&quot;&gt;
        &lt;fs name=&quot;webdata01&quot; device=&quot;/dev/sdb&quot; fstype=&quot;ext4&quot; mountpoint=&quot;/var/www&quot; self_fence=&quot;1&quot;&gt;
          &lt;apache name=&quot;webserver01&quot;/&gt;
        &lt;/fs&gt;     
      &lt;/ip&gt; 
    &lt;/service&gt;
  &lt;/rm&gt;
&lt;/cluster&gt;

</code></pre><p>Save the configuration and scp it to node02:</p><pre><code># ccs_config_validate
# scp ./cluster.conf  node02:/etc/cluster/
</code></pre><p>Start service/Stop Service scripts:</p><pre><code>[root@node01 ~]# cd /usr/local/bin/
[root@node01 bin]# ls
clstart  clstart_all  clstop  clstop_all
[root@node01 bin]# pwd
/usr/local/bin
[root@node01 bin]# cat clstart
#!/bin/sh
service cman start
service rgmanager start
[root@node01 bin]# cat clstart_all 
#!/bin/sh
ssh node01 /usr/local/bin/clstart &amp;
ssh node02 /usr/local/bin/clstart &amp;
wait
[root@node01 bin]# cat clstop
#!/bin/sh
service rgmanager stop
service cman stop
[root@node01 bin]# cat clstop_all
#!/bin/sh
ssh node01 /usr/local/bin/clstop &amp;
ssh node02 /usr/local/bin/clstop &amp;
wait
</code></pre><p>Now start the service:</p><pre><code># clstart_all
# clusvcadm -e service01 -m node01h
</code></pre><p>View the service status:</p><pre><code>[root@node01 bin]# clustat
Cluster Status for cluster01 @ Tue Aug  1 15:57:18 2017
Member Status: Quorate

 Member Name                                                     ID   Status
 ------ ----                                                     ---- ------
 node01h                                                             1 Online, Local, rgmanager
 node02h                                                             2 Online, rgmanager
 /dev/block/8:0                                                      0 Online, Quorum Disk

 Service Name                                                     Owner (Last)                                                     State         
 ------- ----                                                     ----- ------                                                     -----         
 service:service01                                                node01h                                                          started  
</code></pre><p>View <code>ip addr</code> on node01, you could see the 2 address attached to eth0.</p><h3 id=error>Error</h3><p>Emulate an error via:</p><pre><code># pkill -9 corosync
</code></pre><p>Now the node2 will try to detect the heartbeat, if not, it will finally reboot the
node01.</p><pre><code>$ tail -f /var/log/message
Aug  1 15:58:21 node02 corosync[4089]:   [CMAN  ] quorum device re-registered
Aug  1 15:58:21 node02 corosync[4089]:   [QUORUM] Members[2]: 1 2
Aug  1 15:58:21 node02 qdiskd[4148]: Assuming master role
Aug  1 15:58:21 node02 qdiskd[4148]: Writing eviction notice for node 1
Aug  1 15:58:22 node02 qdiskd[4148]: Node 1 evicted
Aug  1 15:58:24 node02 corosync[4089]:   [TOTEM ] A processor failed, forming new configuration.
Aug  1 15:58:26 node02 corosync[4089]:   [QUORUM] Members[1]: 2
Aug  1 15:58:26 node02 corosync[4089]:   [TOTEM ] A processor joined or left the membership and a new membership was formed.
Aug  1 15:58:26 node02 kernel: dlm: closing connection to node 1
Aug  1 15:58:26 node02 corosync[4089]:   [CPG   ] chosen downlist: sender r(0) ip(192.168.4.202) ; members(old:2 left:1)
Aug  1 15:58:26 node02 corosync[4089]:   [MAIN  ] Completed service synchronization, ready to provide service.
Aug  1 15:58:26 node02 rgmanager[4511]: State change: node01h DOWN
Aug  1 15:58:26 node02 fenced[4332]: fencing node node01h
Aug  1 15:58:29 node02 fenced[4332]: fence node01h success
Aug  1 15:58:29 node02 rgmanager[4511]: Taking over service service:service01 from down member node01h
Aug  1 15:58:29 node02 rgmanager[5640]: [ip] Adding IPv4 address 192.168.122.209/24 to eth0
Aug  1 15:58:33 node02 rgmanager[5755]: [fs] mounting /dev/sdb on /var/www
Aug  1 15:58:33 node02 kernel: EXT4-fs (sdb): recovery complete
Aug  1 15:58:33 node02 kernel: EXT4-fs (sdb): mounted filesystem with ordered data mode. Opts: 
Aug  1 15:58:33 node02 rgmanager[5923]: [apache] Checking Existence Of File /var/run/cluster/apache/apache:webserver01.pid [apache:webserver01] &gt; Failed
Aug  1 15:58:33 node02 rgmanager[5945]: [apache] Monitoring Service apache:webserver01 &gt; Service Is Not Running
Aug  1 15:58:33 node02 rgmanager[5967]: [apache] Starting Service apache:webserver01
Aug  1 15:58:34 node02 rgmanager[4511]: Service service:service01 started
</code></pre><p>After reboot, in node01 run <code>clstart</code> to start the cluster.</p><p>Recover the service to node01:</p><pre><code>[root@node01 ~]# clustat 
Cluster Status for cluster01 @ Tue Aug  1 16:02:23 2017
Member Status: Quorate

 Member Name                                                     ID   Status
 ------ ----                                                     ---- ------
 node01h                                                             1 Online, Local, rgmanager
 node02h                                                             2 Online, rgmanager
 /dev/block/8:0                                                      0 Online, Quorum Disk

 Service Name                                                     Owner (Last)                                                     State         
 ------- ----                                                     ----- ------                                                     -----         
 service:service01                                                node02h                                                          started       
[root@node01 ~]# clusvcadm -r service01 -m node01h
Trying to relocate service:service01 to node01h...Success
service:service01 is now running on node01h
[root@node01 ~]# clustat 
Cluster Status for cluster01 @ Tue Aug  1 16:03:38 2017
Member Status: Quorate

 Member Name                                                     ID   Status
 ------ ----                                                     ---- ------
 node01h                                                             1 Online, Local, rgmanager
 node02h                                                             2 Online, rgmanager
 /dev/block/8:0                                                      0 Online, Quorum Disk

 Service Name                                                     Owner (Last)                                                     State         
 ------- ----                                                     ----- ------                                                     -----         
 service:service01                                                node01h                                                          started 
</code></pre><h3 id=configuration-modify>Configuration Modify</h3><p>Use <code>cman_tool version -r</code> command. but not all of the services could be applied
in this way.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2017/07/31/usinglocalrdesktopforacrossingsomething/>UsingLocalRdesktopForAcrossingSomething</a></h1><span class=post-date>Jul 31, 2017<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=mac-address-spoofing>MAC Address Spoofing</h3><p>First you have to cheat your remote machine via changing your own MAC address
from the origin one to the remote box address.</p><p>There are many methods in:<br><a href=https://wiki.archlinux.org/index.php/MAC_address_spoofing>https://wiki.archlinux.org/index.php/MAC_address_spoofing</a></p><p>My method is via changing the systemd-networkd:</p><pre><code>$ pwd
/etc/systemd/network
$ cat 00-default.link 
[Match]
MACAddress=xx:xx:xx:xx:xx

[Link]
MACAddress=xx:xx:xx:xx:xx
NamePolicy=kernel database onboard slot path
</code></pre><p>After your changing, reboot your system.</p><h3 id=iptables-changing>Iptables Changing</h3><p>Add following lines into my own iptables rules:</p><pre><code>sudo iptables -A OUTPUT -o br0 -m state --state ESTABLISHED,RELATED -j ACCEPT
sudo iptables -A OUTPUT -o br0 -p tcp --dport 3389 -j ACCEPT
sudo iptables -A OUTPUT -o br0 -j DROP
sudo iptables -A OUTPUT -o enp0s25 -m state --state ESTABLISHED,RELATED -j ACCEPT
sudo iptables -A OUTPUT -o enp0s25 -p tcp --dport 3389 -j ACCEPT
sudo iptables -A OUTPUT -o enp0s25 -j DROP
sudo iptables -A INPUT -i br0 -m state --state ESTABLISHED,RELATED -j ACCEPT
sudo iptables -A INPUT -i br0 -p tcp --dport 3389 -j ACCEPT
sudo iptables -A INPUT -i br0 -j DROP
sudo iptables -A INPUT -i enp0s25 -m state --state ESTABLISHED,RELATED -j ACCEPT
sudo iptables -A INPUT -i enp0s25 -p tcp --dport 3389 -j ACCEPT
sudo iptables -A INPUT -i enp0s25 -j DROP
</code></pre><p>Now use the <code>rdesktop</code> for viewing the remote desktop, I could get in touch
with the remote machine desktop, now I won&rsquo;t changing the screen for viewing
the remote machine, saving many times.</p><h3 id=iptables-save-permenant>Iptables Save Permenant</h3><p>Edit the service definition files:</p><pre><code># vim /etc/iptables/iptables.rules
*filter
:INPUT DROP [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [251:34691]
-A OUTPUT -o br0 -m state --state ESTABLISHED,RELATED -j ACCEPT
-A OUTPUT -o br0 -p tcp --dport 3389 -j ACCEPT
-A OUTPUT -o br0 -j DROP
-A OUTPUT -o enp0s25 -m state --state ESTABLISHED,RELATED -j ACCEPT
-A OUTPUT -o enp0s25 -p tcp --dport 3389 -j ACCEPT
-A OUTPUT -o enp0s25 -j DROP
-A INPUT -i br0 -m state --state ESTABLISHED,RELATED -j ACCEPT
-A INPUT -i br0 -p tcp --dport 3389 -j ACCEPT
-A INPUT -i br0 -j DROP
-A INPUT -i enp0s25 -m state --state ESTABLISHED,RELATED -j ACCEPT
-A INPUT -i enp0s25 -p tcp --dport 3389 -j ACCEPT
-A INPUT -i enp0s25 -j DROP
COMMIT
</code></pre><p>Enable the service :</p><pre><code># sudo systemctl enable iptables.service
</code></pre><p>This method won&rsquo;t work properly, because libvirtd also add some rules.</p><p>Finally I have to add the scripts in my awesome startup scripts.</p><h3 id=iptables-recovery>Iptables Recovery</h3><p>Recover the default iptables rules via:</p><pre><code>sudo iptables -D OUTPUT -o br0 -m state --state ESTABLISHED,RELATED -j ACCEPT
sudo iptables -D OUTPUT -o br0 -p tcp --dport 3389 -j ACCEPT
sudo iptables -D OUTPUT -o br0 -j DROP
sudo iptables -D OUTPUT -o enp0s25 -m state --state ESTABLISHED,RELATED -j ACCEPT
sudo iptables -D OUTPUT -o enp0s25 -p tcp --dport 3389 -j ACCEPT
sudo iptables -D OUTPUT -o enp0s25 -j DROP
sudo iptables -D INPUT -i br0 -m state --state ESTABLISHED,RELATED -j ACCEPT
sudo iptables -D INPUT -i br0 -p tcp --dport 3389 -j ACCEPT
sudo iptables -D INPUT -i br0 -j DROP
sudo iptables -D INPUT -i enp0s25 -m state --state ESTABLISHED,RELATED -j ACCEPT
sudo iptables -D INPUT -i enp0s25 -p tcp --dport 3389 -j ACCEPT
sudo iptables -D INPUT -i enp0s25 -j DROP
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2017/07/31/readingtipsonlinuxsystemarchitecture/>ReadingTipsOnLinuxSystemArchitecture</a></h1><span class=post-date>Jul 31, 2017<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=on-this-book>On This Book</h3><p>Borrowed from lab, written via a janpanese author.<br><img src=/images/2017_07_31_09_20_33_1054x739.jpg alt=/images/2017_07_31_09_20_33_1054x739.jpg>
This article will record the reading tips on Chapter 2(libvirtd related).</p><h3 id=network-configuration>Network Configuration</h3><p>Edit the netoworking definition xml:</p><pre><code>$ cat internal.xml
&lt;network&gt;
	&lt;name&gt;internal&lt;/name&gt;
	&lt;bridge name='virbr8'/&gt;
&lt;/network&gt;
$  cat external.xml
&lt;network&gt;
	&lt;name&gt;external&lt;/name&gt;
	&lt;bridge name='virbr9'/&gt;
&lt;/network&gt;
</code></pre><p>Define the networking via following commands:</p><pre><code>$ sudo virsh net-define external.xml
Network external defined from external.xml

$ sudo virsh net-autostart external
Network external marked as autostarted

$ sudo virsh net-start external
Network external started

$ libvirt sudo virsh net-list
 Name                 State      Autostart     Persistent
----------------------------------------------------------
 default              active     no            yes
 external             active     yes           yes
 internal             active     yes           yes
 kubernetes           active     yes           yes
</code></pre><p>View the configuration in virt-manager:</p><p><img src=/images/2017_07_31_09_34_07_495x298.jpg alt=/images/2017_07_31_09_34_07_495x298.jpg></p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2017/07/27/correcthugodate/>CorrectHugoDate</a></h1><span class=post-date>Jul 27, 2017<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=problem>Problem</h3><p><img src=/images/2017_07_27_16_08_05_1361x260.jpg alt=/images/2017_07_27_16_08_05_1361x260.jpg></p><h3 id=reason>Reason</h3><p>This is because hugo upgrade to a new version <code>0.25.1</code>, while this new version
won&rsquo;t give the default value of date in newly created markdown file.</p><h3 id=solution>Solution</h3><p>Edit the <code>themes/hyde-a/archetypes/default.md</code>, add following items:</p><pre><code>+++
title = &quot;&quot;
date = &quot;{{ .Date }}&quot;
description = &quot;&quot;
keywords = [&quot;Linux&quot;]
categories = [&quot;Technology&quot;]
+++
</code></pre><p>Now you could re-new your configuration, and then your blog will acts OK.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2017/07/27/createrhel6customizediso/>CreateRHEL6CustomizedISO</a></h1><span class=post-date>Jul 27, 2017<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=目的>目的</h3><p>根据用户自定义配置，自动从ISO安装出整个系统。</p><h3 id=准备材料>准备材料</h3><p>RHEL 6.6安装光盘, <code>x86_64</code>版本。
自定义kickstart文件，用于自定义分区/用户/密码/安装包等<br>红帽系列操作系统(用于制作光盘镜像，已验证Redhat7.3)</p><h3 id=步骤>步骤</h3><ol><li>创建目录用于挂载安装光盘和自定义光盘,
其中<code>/media/bootiso</code>用于挂载安装光盘，
<code>/media/bootisoks</code>用于存放自定义光盘内容:</li></ol><pre><code>$ mkdir -p /media/bootiso /media/bootisoks
</code></pre><ol start=2><li>拷贝安装内容到自定义光盘目录:</li></ol><pre><code>$ sudo mount -t iso9660 -o loop DVD.iso /media/bootiso
$ cp -r /media/bootiso/* /media/bootisoks/
$ chmdo -R u+w /media/bootisoks
$ cp /media/bootiso/.discinfo /media/bootisoks
$ cp /media/bootiso/.discinfo /media/bootisoks/isolinux
</code></pre><ol start=3><li>拷贝自定义的ks文件到isolinux目录下:</li></ol><pre><code>$ cp YourKickStartFile.ks /media/bootisoks/isolinux
</code></pre><ol start=4><li>配置引导选项:</li></ol><pre><code>$ vim /media/bootisoks/isolinux.cfg
initrd=initrd.img ks=cdrom:/isolinux/ks.cfg
</code></pre><ol start=5><li>创建ISO文件:</li></ol><pre><code># mkisofs -r -T -V &quot;MYISONAME&quot; -b isolinux/isolinux.bin -c isolinux/boot.cat
-no-emul-boot -boot-load-size 4 -boot-info-table -o ../boot.iso .
</code></pre><p>经历此五个步骤以后，即可得到我们定制好的ISO，用此ISO即可安装出我们自定义好的系统.</p><h3 id=kickstart示例文件>kickstart示例文件:</h3><p>安装了基本桌面、中文支持等。</p><pre><code>#platform=x86, AMD64, or Intel EM64T
#version=DEVEL
# Firewall configuration
firewall --disabled
# Install OS instead of upgrade
install
# Use network installation
#url --url=&quot;http://10.7.7.2/CentOS&quot;
cdrom
# Root password
rootpw --iscrypted xxxxxxxxxxxxxxxxxxxx
# System authorization information
auth  --useshadow  --passalgo=sha512
# Use graphical install
graphical
firstboot --disable
# System keyboard
keyboard us
# System language
lang en_US
# SELinux configuration
selinux --disabled
# Installation logging level
logging --level=info

# System timezone
timezone  Asia/Hong_Kong
# System bootloader configuration
bootloader --location=mbr
# Clear the Master Boot Record
zerombr
# Partition clearing information
clearpart --all  
# Disk partitioning information
part swap --fstype=&quot;swap&quot; --size=1024
part / --asprimary --fstype=&quot;ext4&quot; --grow --size=1

%packages
@basic-desktop
@chinese-support
@internet-browser
@x11
-ibus-table-cangjie
-ibus-table-erbi
-ibus-table-wubi

%end
</code></pre><p>其中<code>rootpw</code>以后的字段可以通过以下命令得到:</p><pre><code>$ openssl passwd -1 &quot;Your_Password_Here&quot;
</code></pre><h3 id=kscfg的另一种构建方法>ks.cfg的另一种构建方法</h3><p>在安装完的每一台机器上，都可以看到/root/ana&mldr;ks文件，编辑此文件即可得到我们定制化的kickstart配置。</p></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/89/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/89/>89</a></li><li class="page-item active"><a class=page-link href=/page/90/>90</a></li><li class=page-item><a class=page-link href=/page/91/>91</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/238/>238</a></li><li class=page-item><a href=/page/91/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/238/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>